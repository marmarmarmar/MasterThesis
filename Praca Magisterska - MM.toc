\select@language {polish}
\contentsline {chapter}{\numberline {1}Historia}{7}
\contentsline {section}{\numberline {1.1}Pocz\k atki i perceptron}{7}
\contentsline {section}{\numberline {1.2}Zima sztucznej inteligencji}{7}
\contentsline {section}{\numberline {1.3}Dlaczego perceptron zawi\'od\IeC {\l }?}{8}
\contentsline {section}{\numberline {1.4}Algorytm wstecznej propagacji - odrodzenie i ponowna zima}{8}
\contentsline {section}{\numberline {1.5}Dlaczego sieci ponownie zawiod\IeC {\l }y?}{9}
\contentsline {chapter}{\numberline {2}Matematyczne podstawy algorytm\'ow}{11}
\contentsline {section}{\numberline {2.1}Uczenie}{11}
\contentsline {subsection}{\numberline {2.1.1}Formalna definicja uczenia}{11}
\contentsline {subsection}{\numberline {2.1.2}Uczenie z nadzorem}{12}
\contentsline {subsection}{\numberline {2.1.3}Uczenie bez nadzoru}{13}
\contentsline {subsection}{\numberline {2.1.4}Generalizacja oraz przeuczenie}{14}
\contentsline {subsubsection}{Zbiory walidacyjne i testowe}{15}
\contentsline {subsubsection}{Walidacja krzy\.zowa (ang. \textit {cross-validation}}{15}
\contentsline {subsection}{\numberline {2.1.5}Optymalizacja gradientowa}{16}
\contentsline {subsubsection}{Redukcja sta\IeC {\l }ej uczenia w wypadku wysokiej i sta\IeC {\l }ej warto\'sci funkcji kosztu (ang. \textit {learning rate reduction on plateau})}{16}
\contentsline {subsection}{\numberline {2.1.6}Stochastyczna optymalizacja gradientowa}{16}
\contentsline {subsection}{\numberline {2.1.7}Metaoptymalizacja}{17}
\contentsline {section}{\numberline {2.2}Sztuczne Sieci neuronowe}{17}
\contentsline {subsection}{\numberline {2.2.1}Biologiczna motywacja}{18}
\contentsline {subsection}{\numberline {2.2.2}Regresja logistyczna}{18}
\contentsline {subsubsection}{Sigmoidalna funkcja aktywacji}{18}
\contentsline {subsubsection}{Logistyczna funkcja kosztu - (ang. \textit {log-loss})}{19}
\contentsline {subsubsection}{Uog\'olnienie funkcji \textit {log-loss} do zagadnienia multiklasyfikacji}{19}
\contentsline {section}{\numberline {2.3}Klasyczne sieci neuronowe oraz algorytm wstecznej propagacji}{19}
\contentsline {subsection}{\numberline {2.3.1}Topologie sieci}{19}
\contentsline {subsection}{\numberline {2.3.2}Wielowarstwowa sie\'c neuronowa jako uniwersalny aproksymator}{20}
\contentsline {subsection}{\numberline {2.3.3}Algorytm wstecznej propagacji}{21}
\contentsline {subsubsection}{Zjawisko Covariance shift i technika Batch Normalization}{23}
\contentsline {subsection}{\numberline {2.3.4}Interpretacja probabilistyczna sieci neuronowych}{23}
\contentsline {subsection}{\numberline {2.3.5}Regularyzacja sieci neuronowych}{24}
\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{2}$}{24}
\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{1}$}{25}
\contentsline {subsection}{\numberline {2.3.6}Czym r\'o\.zni\k a si\k e powy\.zsze kary?}{25}
\contentsline {subsection}{\numberline {2.3.7}Intuicje dotycz\k ace warstw sieci neuronowych}{25}
\contentsline {subsection}{\numberline {2.3.8}Reprezentacja rozproszona}{26}
\contentsline {subsubsection}{G\IeC {\l }\k eboka hierarchiczna struktura przyczyn\k a sukcesu algorytm\'ow z rodziny Deep Learning}{27}
\contentsline {chapter}{\numberline {3}Rozw\'oj metod uczenia}{29}
\contentsline {section}{\numberline {3.1}Sieci konwolucyjne oraz LeNet}{29}
\contentsline {subsection}{\numberline {3.1.1}Inwariancja}{29}
\contentsline {subsection}{\numberline {3.1.2}Sieci konwolucyjne}{30}
\contentsline {subsection}{\numberline {3.1.3}Pooling}{31}
\contentsline {subsection}{\numberline {3.1.4}LeNet}{31}
\contentsline {section}{\numberline {3.2}Autoenkodery, sie\'c Hintona i Salakhudinowa}{32}
\contentsline {section}{\numberline {3.3}Sie\'c Kryzhevskiego}{32}
\contentsline {section}{\numberline {3.4}GoogLeNet}{32}
\contentsline {section}{\numberline {3.5}Inne sieci}{32}
\contentsline {chapter}{\numberline {4}Moja implementacja}{33}
\contentsline {chapter}{Bibliografia}{35}
