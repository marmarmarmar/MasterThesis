\documentclass{pracamgr}

\usepackage[latin2]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}
\usepackage{polski}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[boxed, onelanguage]{algorithm2e}

\author{Marcin Mo¿ejko}
\nralbumu{262793}
\title{Podstawy metod g³êbokiego uczenia wraz z przyk³adami zastosowañ}

\tytulang{Foundations od deep learning methods with examples of applications}

\kierunek{Matematyka}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podaæ tytu³/stopieñ imiê i nazwisko opiekuna
% Instytut
% ew. Wydzia³ ew. Uczelnia (je¿eli nie MIM UW))
\opiekun{Dr Ewy Szczurek}

% miesi±c i~rok:
\date{Wrzesieñ 2017}

%Podaæ dziedzinê wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
11.4 Sztuczna inteligencja\\ 
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  D.127. Blabalgorithms\\
  D.127.6. Numerical blabalysis,\\
  TODO}

% S³owa kluczowe:
\keywords{deep learning, machine learning, uczenie maszynowe, sztuczne sieci neuronowe, 
sieci konwolucyjne}

% Tu jest dobre miejsce na Twoje w³asne makra i~¶rodowiska:
\newtheorem{defi}{Definicja}[section]
\newtheorem{przyklad}{Przyk³ad}[section]
\newtheorem{uwaga}{Uwaga}[section]
\newtheorem{twierdzenie}{Twierdzenie}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
W ostatnim czasie, w¶ród specjalistów zajmuj±cych siê uczeniem maszynowym, bardzo eksplorowanym tematem sta³o siê tzw. g³êbokie uczenie, czyli uczenie z wykorzystaniem g³êbokich (tzn. maj±cych wiele warstw) sieci neuronowych. W swojej pracy przedstawie matematyczne podstawy sukcesu stoj±cych za tym algorytmów.
\end{abstract}

\tableofcontents
	%\listoffigures
%\listoftables

\chapter*{Wprowadzenie}

W ostatnim czasie, jednym z tematów które najbardziej poch³aniaj± specjalistów od \textit{Machine Learningu} sta³y siê tzw. metody g³êbokiego uczenia (\textit{deep learning}). Co chwila mo¿emy us³yszeæ informacjê, ¿e wed³ug naukowców wykorzystuj±cych te metody, algorytmy przez nich otrzymane pokona³y cz³owieka w zagadnieniu, które do tej pory wydawa³o siê byæ tylko w zasiêgu ludzkiego umys³u (\cite{twarze}, \cite{znaki}). To co ³±czy wiêkszo¶æ tych zagadnieñ i jest w mojej opinii kluczowe do zrozumienia fenomenu \textit{deep learningu} to :

\begin{itemize}
	\item ogromna ilo¶æ danych (czêsto bez ¿adnych etykiet),
	\item du¿a problematyczno¶æ w zakodowaniu problemu w klasycznym jêzyku algorytmicznym,
	\item zbiór interesuj±cych nas danych jest niezwykle ma³y w stosunku do przestrzeni z której pochodz± dane.
\end{itemize}

Spróbujmy przyjrzeæ siê tym podpunktom. Rozwa¿aj±c np. problem rozpoznawania przedmiotów na filmach, mo¿emy natrafiæ na ciekawy symptom. Dziêki wieloletniej historii zapisu cyfrowej, przez lata ludzko¶æ zebra³a na rozmaitych no¶nikach danych niezmierzone ilo¶ci danych z kamer. Dziêki temu wspó³cze¶ni badacze maj± u³atwione zadanie przy budowaniu algorytmów do rozpoznawania filmów - mog± wykorzystaæ tak zdobyta wiedzê do poprawienia rezultatów uzyskiwanych przez ich programy.

Kolejny podpunkt, czyli problematyczno¶æ, mo¿emy doskonale zobrazowaæ przez przyk³ad zaprojektowania sieci rozpoznaj±cej czy dana recenzja na portalu spo³eczno¶ciowym zawiera pozytywn± opiniê o temacie b±d¼ nie. Niech czytelnik pokusi siê o zapisanie na kartce rzeczy, które algorytm powinien sprawdziæ aby uzyskaæ poprawn± odpowied¼. Krótka inspekcja powinna u¶wiadomiæ jak karko³omne zadanie staje przed badaczami, którzy siê tym zajmuj±. Wyniki ostatnich algorytmów pokazuj± jednak, ¿e problem ten nie przekracza zasiegu wspó³czesnych technologii (\cite{sentiment}).

Ostatnie zagadnienie - najlepiej zobrazowaæ liczbami. Za³ó¿my, ¿e rozwa¿amy obraz który posiada 28 x 28 pikseli oraz, dla uproszczenia, jest czarnobia³y. Ilo¶æ wszystkich mo¿liwych obrazków tego typu przekracza niewyobra¿alnie liczbê atomów we wszech¶wiecie. Natomiast liczba obrazków które prezentuj± nam co¶ interesuj±cego (np. znany problem rozpoznawania cyfr \cite{mnist}) jest liczb± zdecydowanie mniejsz±. Mo¿na powiedzieæ, ¿e interesuj±cy nas zbiór jest niczym galaktyka po¶ród otaczaj±cego j± szumu i pustki.

W swojej pracy chcia³bym przedstawiæ w jaki sposób poradzono sobie z powy¿szymi zagadnieniami. Zamierzam zacz±æ od historii powstania tych algorytmów, któr± uwa¿am za bardzo wa¿n± w zrozumieniu istoty ich dzia³ania, by potem zg³êbiæ podstawy matematycznych narzêdzi stoj±cych za sukcesem \textit{deep learningu}. W kolejnych rozmiarach zamierzam, na podstawie historycznych algorytmów, przedstawiæ to, co stanowi esencjê sukcesu metod g³êbokiego uczenia. Wydajê mi siê, ¿e ta droga wyt³umaczy najlepiej, dlaczego niemal ka¿dy szanuj±cy siê o¶rodek naukowy posiada dzi¶ grupê zajmuj±c± siê metodami g³êbokiego uczenia.

\chapter{Historia}

\section{Pocz±tki i perceptron}

Aby dobrze zrozumieæ historiê metod \textit{deep learning} nale¿y w moim mniemaniu choæ pobie¿nie zg³êbiæ historiê powstania i rozwoju metod opartych na tym paradygmacie. Mo¿na powiedzieæ, ¿e historia ta zaczê³a siê w momencie odkrycia przez biologów neuronalnej natury mózgu, jednak pocz±tki algorytmicznych sieci neuronowych, których najwy¿sz± emanacj± s± g³êbokie sieci, datuje siê na koniec lat 60' XX w., gdy Frank Rosenblat przygotowa³ mechaniczno - elektryczny perceptron \cite{perceptron}, czyli realizacje modelu neuronu McCullocha-Pittsa \cite{mccul-pitts}. Choæ teoretyczne podstawy opracowane przez wspomniany duet naukowców powsta³y ponad 25 lat wcze¶niej, to rewolucyjnym rozwi±zaniem, zastosowanym w przypadku koncepcji Rosenblata by³ efektywny i skuteczny model uczenia, który przyczyni³ siê do wieloletniego, bujnego rozwoju zastosowañ opartych na tej koncepcji. Warto tutaj podkre¶liæ motyw, który bêdzie powtarza³ siê przy omawaniu kolejnych punktów, biologiczne inspiracje, które doprowadzi³y do rozwoju algorytmicznych rozwi±zañ.

\section{Zima sztucznej inteligencji}

I wtedy, gdy wydawa³o siê, ¿e perceptron to narzêdzie niemal idealne do wszystkich zadañ, na horyzoncie pojawi³ siê ogromny problem. Jako narzêdzie, wynalazek Rosenblata s³u¿y³ min. do rozpoznawania obiektów na obrazkach. Okaza³o siê jednak, ¿e mniemanie o jego  skuteczno¶ci jest znacznie przesadzone. W wiêkszo¶ci wypadków bowiem okaza³o siê, ¿e to co zostaje wykrywane to jaka¶ istotnie prosta cecha, która towarzyszy obrazom interesuj±cych badaczy, a nie obiekty obecno¶æ których algorytm mia³ wykrywaæ. I tak np. zamiast statków algorytm wykrywa³ ciemne plamy po ¶rodku zdjêcia, a zamiast czo³gów - zaciemnienia w kszta³cie lasu, w których najczê¶ciej maszyny by³y fotografowane \cite{minsky-papert}. Co wiêcej, w Marvin Minsky oraz Seymour Papert \cite{minsky-papert} pokazali du¿o mocniejsze ograniczenia na to, co mog³o byæ efektywnie rozpoznane przez perceptron. Okaza³o siê, ¿e wyuczenie prostej, logicznej funkcji XOR przekracza mo¿liwo¶ci wynalazku Rosenblata. Co wiêcej - uogólnienie tego wyniku pokaza³o, ¿e w szczególno¶ci, niezale¿enie od rozmiaru oraz typu zbioru treningowego, nie mo¿na perceptronu nauczyæ rozpoznawania wzoru, który by³by zamkniêty na wszystkie mo¿liwe translacje (przesuniêcia) na obrazku. Fakt ten wywo³a³ tak ogromny szok w¶ród badaczy, ¿e ilo¶æ naukowców oraz funduszy przeznaczonych na badania drastycznie zmala³a, a niski poziom zainteresowania utrzymywa³ siê przez kolejne 10 lat. Dlatego w³a¶nie ten okres nazywa siê w historii \textit{machine learningu} zim± sztucznej inteligencji.

\section{Dlaczego perceptron zawiód³?}

Odpowied¼ na pytanie dlaczego perceptron zawiód³, nale¿y podzieliæ na dwa fragmenty : jakie by³y matematyczne powody dla których algorytm nie podo³a³ oczekiwaniom oraz dlaczego oczekiwania które zawiód³ by³y tak ogromne. Odpowied¼ na pierwsz± czê¶æ jest prozaiczna. Model McCullocha-Pittsa zastosowany przez Rosenblata, ogranicza dzia³anie algorytmu do rozwa¿ania znaku jaki przyjmuje funkcjona³ afiniczny dla zadanego wektora w $\mathbb{R}^n$. Krótka inspekcja tego faktu pokazuje, ¿e istotnie rozró¿niane wedle tego algorytmu mog± byæ tylko zbioru, które s± separowalne liniowo, tzn. istnieje podprzestrzeñ kowymiaru 1, która je istotnie oddziela. Odpowied¼ na drug± czê¶æ, czyli spo³eczne rozczarowanie, które poci±gn±³ za sob± krach zaufania do perceptronu do dzi¶ ciê¿ko mi zrozumieæ. Byæ mo¿e jest to wynika³o to z ogólnego wówczas zachwytu nad nowymi technologiami wynikaj±cego z wynalezienia komputerów i ogólnie elektroniki? (\cite{lem}) Z ca³± pewno¶ci± po latach - patrz±c na matematyczne podstawy algorytmu Rosenblata - mo¿emy stwierdziæ, ¿e nadmierne oczekiwania w stosunku do tak prostego narzêdzia okaza³y siê po prostu naiwne. Co wiêcej - naiwno¶æ ta nie le¿y tylko w matematycznej podstawie algorytmu, ale tak¿e w biologicznej nieadekwatno¶ci. Skoro naukowcy powo³ywali siê czê¶ciowo na na¶ladowanie mózgu nie sposób dostrzec prostego faktu : organ ten u cz³owieka posiada ok. $10^{11}$ neuronów i z tyle razy mniejszej liczby sk³ada siê perceptron. 

\section{Algorytm wstecznej propagacji - odrodzenie i ponowna zima}

Przez kolejne lata badacze rozwijaj±cy algorytmy zwi±zane z sieciami neuronowymi próbowali pokonaæ przedstawione powy¿ej przeszkody. Naturalnym rozwi±zaniem wydawa³o siê zbudowanie sieci sk³adaj±ce siê z wiêkszej ilo¶ci - tak neuronów, jak i ich warstw. Rozwi±zanie to napotka³o jednak na dosyæ powa¿ny problem - o ile uczenie perceptronu by³o zadaniem banalnie prostym, to wyuczenie sieci o wiêkszej ilo¶ci wartstw okaza³o siê zadaniem o wiele trudniejszym. Podstawowy problem wynika z tego, ¿e aby otrzymaæ now± jako¶æ i przezwyciê¿yæ stare problemy, nale¿y zastosowaæ inn± ni¿ liniow± funkcjê obliczaj±c± wynik neuronu w zale¿no¶ci od informacji której do niego wprowadzimy. Konieczno¶æ ta wyp³ywa ze znanego faktu, ¿e z³o¿enie wielu funkcji liniowych, jest tak¿e funkcj± liniow±. Zatem niezale¿enie od tego ile neuronów bêdzie sk³ada³o siê na strukturê naszej sieci - uzyskane narzêdzie bêdzie równowa¿ne klasycznemu perceptronowi. 

Jednak wprowadzenie nieliniowych tzw. funkcji aktywacji rodzi inne problemy - jak mianowicie skutecznie przeprowadziæ proces uczenia naszego modelu gdy analityczne rozwi±zania na ogó³ s± nieosi±galne? I tutaj pomoc przysz³a z najmniej spodziewanej strony, a mianowicie pochodz±cej z klasycznego rachunku prawdopodobieñstwa regu³y ³añcuchowej. Algorytm, (Rumelhart et al. \cite{backprop}) którego istota dzia³ania polega na obliczeniach gradientu b³êdu, a nastêpnie aktualizowania odpowiednich parametrów na podstawie ich wp³ywu na omylno¶æ modelu (wp³yw ten jest oblicznany w³a¶nie przez regu³ê ³añcuchow±) przyniós³ fantastyczne rezultaty. Ze wzglêdu na to, ¿e b³±d obliczany przy dzia³aniu sieci, propaguje siê nastêpnie na odpowiednie parametry, nazwano go algorytmem wstecznej propagacji.

Od momentu prezentacji algorytmu (1986 r.) nast±pi³ ponowny, bujny rozwój sieci neuronowych, który zaowocowa³ powstaniem wielu technik, które s± popularne do dzi¶ (np. \cite{backprop, recurrent}. Co jednak charakterystyczne, entuzjazm towarzysz±cy badaniom stanowi³ tylko cieñ euforii która towarzyszy³a wynalezieniu perceptronu. Po ok. 10 latach (pocz±tek lat 90' XXw.), z powodu zbyt wolnego rozwoju technologii i algorytmów, a tak¿e nieuzyskiwania oczekiwanych rezultatów, przyszed³ czas ponownego rozczarowania, podczas którego ponownie niemal skre¶lono sieci neuronowe z listy obiecuj±cych kierunków rozwoju. (\cite{vapnik-bet}) Traktowano je bardziej jako wzorowan± na przyrodzie ciekawostkê, ni¿ drogê która mo¿e przynie¶æ rozwi±zanie dla podstawowych problemów sztucznej inteligencji.

\section{Dlaczego sieci ponownie zawiod³y?}

Ponowna zima jednak, podobnie jak i entuzjazm który towarzyszy³ powrotowi sieci neuronowych do ³ask, okaza³a siê du¿o mniej intensywna ni¿ poprzednia. Spróbujmy jednak przyjrzeæ siê podstawowym przyczynom, które sprawi³y, ¿e modele te zawiod³y oczekiwania rozwijaj±cych je badaczy.

Pierwsz± przyczyn± stanowi³y ponownie zbyt du¿e oczekiwania. Jakkolwiek na prze³omie lat 90' XX w. udowodniono, ¿e sieæ z jedn± tzw. warstw± ukryt± jest w stanie nauczyæ siê niemal ka¿dej funkcji $f:\mathbb{R}^n\rightarrow \mathbb{R}$ ci±g³ej o zwartym no¶niku (\cite{cybenko, hornik}, to nie do koñca zdawano sobie sprawe, ¿e zadanie nauczenia chocia¿by rozpoznawania obrazu lub ludzkiej mowy, choæ mo¿liwe, mog± byæ bardzo trudne. W szczególno¶ci, w dowodach powy¿szego faktu, niedostrze¿ono alarmuj±cego wzrostu z³o¿ono¶ci problemu wraz ze wzrostem jego skomplikowania. Doprowadzi³o to do nieuzasadnionego rozczarowania tym, ¿e \textit{de facto} nie mo¿emy nauczyæ sieci wszystkiego na pamiêæ. 

Drug± przyczynê, zwi±zan± w³a¶nie ze wspomnianym powy¿ej problemem, stanowi³y niedostateczne warunki sprzêtowe, które uniemo¿liwia³y wykorzystanie potencja³u drzemi±cego w algorytmie. Niedostateczno¶æ ta nie wynika³a tylko ze zbyt ma³ych mo¿liwo¶ci obliczeniowych, ale tak¿e znacznego niedoboru danych (tak w sensie ich zebrania, jak i przechowywania). Najwiêksze zebrane zbiory treningowe nie liczy³y, na pocz±tku lat 90', wiêcej ni¿ 100 000 elementów. Z perspektywy wyuczenia chocia¿by zagadnieñ zwi±zanych z obrazem lub mow± ludzk± liczba ta jawi siê jako niemal¿e ¶miesznie ma³a. Problemy te jednak rozwi±za³ czas - wraz z rozwojem technologii pojawi³y siê tak i nowoczesne komputery, które przyspieszy³y proces uczenia oraz ewaluacji, jak i ogromne, zró¿nicowane zbiory danych, które sprawi³y, ¿e problemy o rozwi±zaniu których marzyli eksperci od sztucznej inteligencji, znalaz³y siê jak najbardziej w zasiêgu ludzkich mo¿liwo¶ci.

\chapter{Matematyczne podstawy algorytmów} \label{matpod}

\section{Uczenie}

Wszystkie algorytmy opisane w tej pracy nale¿± do rodziny algorytmów ucz±cych. Aby móc o nich mówiæ musimy zatem wprowadziæ formaln± definicjê uczenia. Zrobimy to w kolejnej podsekcji. Pó¼niej przedstawimy trzy najczêstsze przyk³ady uczenia czyli uczenie z i bez nadzoru, a tak¿e uczenie z pó³nadzorem. 

\subsection{Formalna definicja uczenia} \label{defucz}

Trzy rzeczy s± absolutnie niezbêdne aby mówiæ o o algorytmach ucz±cych. Pierwsza z nich to zbiór treningowy. Nale¿y o nim my¶leæ jako o zbiorze danych, które znamy, a które chcemy wykorzystaæ jako podstawê do uczenia przy pomocy naszego algorytmu. Drug± z nich jest zbiór modeli. Efektem uczenia ma byæ efektywnie opisuj±cy nasze dane model, aby jednak móc taki znale¼æ, musimy ustaliæ pewien zbiór mo¿liwych modeli spo¶ród którego bêdziemy go wybieraæ. Koniecznym jest tak¿e ustalenie, co oznacza, ¿e nasze dane s± efektywnie opisywane przez wybrany przez nas opis - do tego s³u¿y funkcja kosztu. Intuicyjnie, funkcj± kosztu nazywamy nieujemn± funkcjê rzeczywist±, malej±c± wraz ze wzrostem efektywno¶ci opisywanych danych. Wszystkie powy¿sze intuicje sk³adaj± siê na poni¿sz± definicjê. 

\begin{defi}
Niech $\mathcal{X}$ bêdzie dowolnym zbiorem (tzw. \textbf{treningowym}), $\Theta$ - zbiorem modeli, a 
$$J_{\mathcal{X}} : \Theta \rightarrow \mathbb{R}_{+}\cup \{0\}$$ funkcj± kosztu. \textbf{Uczeniem} bêdziemy nazywaæ algorytm maj±cy na celu odnalezienie :

$$ \theta_{\mathcal{X}} = \mathtt{argmin}_{\theta \in \Theta} J_{\mathcal{X}}\left(\theta\right).$$

\end{defi}

Przy okazji tej definicji nale¿y podnie¶æ kilka istotnych kwestii, które s± ¶ci¶le zwi±zane z procesem uczenia, a które nale¿± do zagadnieñ zaawansowanej filozofii nauki i daleko przekraczaj± zakres poni¿szej pracy. Pierwsz± rzecz± na któr± nale¿y zwróciæ uwagê jest to, ¿e $\theta_{\mathcal{X}}$ to \textit{de facto} rozwi±zanie pewnego problemu optymalizacyjnego i nie nale¿y do gestii algorytmu ucz±cego rozs±dzanie czy opis wyznaczany przez wybrany model mie¶ci siê w granicach które uznajemy za rozs±dne, b±d¼ nie. Wybór rodziny modeli, a tak¿e funkcji kosztu musi poprzedziæ g³êboka analiza, a tak¿e refleksja nad problemem, aby dokonany przez nas wybór nie okaza³ siê niesatysfakcjonuj±cy. Nale¿y mieæ tak¿e w pamiêci, ¿e zgodnie z aktualnie przyjêt± filozofi± (\cite{popper}), wybrany przez nas model, nawet w kontek¶cie uzyskania pozytywnych wyników, nale¿y traktowaæ jako niesfalsyfikowany, a nie prawdziwy. Jest to o tyle istotne, ¿e wypracowane przez nas rozwi±zanie zale¿y w bardzo istotny sposób od zbiory danych, który wykorzystujemy przy uczeniu - nap³yw kolejnych, mo¿e doprowadziæ do weryfikacji tak konkrentego $\theta_{\mathcal{X}}$ jak i samej rodziny $\Theta$.

\subsection{Uczenie z nadzorem}

Przez uczenie z nadzorem rozumiemy przypadek, w którym mo¿emy wyró¿niæ dwa zbiory $\mathbb{X}_\mathcal{X}$ (zbiór argumentów) oraz $\mathbb{Y}_\mathcal{X}$ (zbiór warto¶ci) takie, ¿e $\mathcal{X} \subset 
\mathbb{X}_\mathcal{X} \times \mathbb{Y}_\mathcal{X}$. Oznacza to, ¿e na ka¿dy element zbioru treningowego mo¿emy patrzyæ jak na parê argument - warto¶æ, a rodzinê modeli $\Theta$ z których wybieramy okre¶liæ jako rodzinê funkcji najlepiej przybli¿aj±c± relacjê jak± na $\mathbb{X}_\mathcal{X} \times \mathbb{Y}_\mathcal{X}$ generuje zbiór $\mathcal{X}$. Dobr± ilustracjê do tego zagadnienia ilustruje poni¿szy przyk³ad.

\begin{przyklad}
W poni¿szym przyk³adzie za zbiór treningowy $\mathcal{X}$ uznamy punkty zaznaczone czarnymi okrêgami, przyjmuj±c, ¿e pierwsza wspó³rzêdna odpowiada argumentom, a druga - warto¶ciom. Funkcj± kosztu bêdzie funkcja :

$$J\left(\theta\right) = \sum_{x \in \mathbb{X}_\mathcal{X}}\left(y_x - f_{\theta}(x) \right)^2,$$

gdzie $y_x$ to warto¶æ odpowiadaj±ca argumentowi $x$, a $f_\theta$ to funkcja odpowiadaj±ca modelowi $\theta$. Na poni¿szym obrazku kolorem czerwonym oznaczono $\theta_\mathcal{X}$ w przypadku gdy $\Theta$ to zbiór funkcji liniowych, zielonym - gdy $\Theta$ to zbiór funkcji sze¶ciennych, natomiast niebieskiem - przypadek gdy $\Theta$ to zbiór funkcji wielomianowych stopnia co najwy¿ej 20. 	

\begin{center}
\includegraphics[scale = 0.5]{Regresja1.png}
\end{center}

\end{przyklad}

Warto podkre¶liæ, ¿e wybrana przeze mnie ogólno¶æ definicji ma swoje g³êbokie uzasadnienie. Zwyczajowo techniki uczenia z nadzorem dzieli siê na dwie grupy : techniki regresji (gdy $\mathbb{Y}_{\mathcal{X}} = \mathbb{R}^n$ dla pewnego $n\in\mathbb{N}$) oraz techniki klasyfikacji (gdy $\mathbb{Y}_{\mathcal{X}}$ jest zbiorem dyskretnym). W swoim opisie podkre¶lam jednak to, ¿e tym co \textit{de facto} jest szukane, to zale¿no¶æ funkcyjna, poniewa¿ pozwala nam to na znacznie szerszy dobór struktur warto¶ci (co np. gdy $\mathbb{Y}_{\mathcal{X}}$ to zbiór zdañ w jêzyku angielskim), a tak¿e unika konfuzji gdy w niektórych przypadkach wybrane przez nas w³asno¶ci $\mathbb{Y}_{\mathcal{X}}$ mog± prowadziæ do nieefektywnego uczenia lub rezultatów innych ni¿ oczekiwane.

\subsection{Uczenie bez nadzoru}

Przez uczenie bez nadzoru bêdziemy rozumieæ tak± formê uczenia, w której $\Theta$ jest zbiorem mo¿liwych, po¿ytecznych reprezentacji zbioru danych $\mathcal{X}$. Funkcja kosztu $J_{\mathcal{X}}$ okre¶la w tym przypadku jak efektywny opis danych gwarantuje nam model $\theta$. Czêsto zmiana reprezentacji danych stanowi konieczny krok przy analizie danych. Zauwa¿my np., ¿e z perspektywy komputera obraz z kamery telewizyjnej to ci±g elementów ze zbioru $\mathbb{R}^{6220800}$ (przy za³o¿eniu jako¶ci FullHd z kodowaniem RGB), co czyni problem komputerowej analizy takiego obrazu praktycznie nierozwi±zywalnym. Na poni¿szym przyk³adzie zobaczmy jak wygl±da dobór reprezentacji przy uczeniu bez nadzoru.

\begin{przyklad}

Poni¿ej przedstawimy przyk³ad tzw. uczenia rozmaito¶ci (\textit{manifold learning} \cite{manifold-learning}) metod± t-SNE. W przyk³adzie tym:
$$\mathcal{X} = \left\{x_1, x_2, \dots, x_k \right\} \subset \mathbb{R}^{n},$$
oraz
$$\Theta = \left\{\left\{\theta_1, \theta_2, \dots, \theta_k\right\} : \theta_1, \theta_2, \dots, \theta_k \in \mathbb{R}^{m}\right\}.$$

Aby zdefiniowaæ funkcjê b³êdu potrzebne bêd± nam pomocnicze wyra¿enia:

$$p_{j|i} = \frac{exp\left(-||x_i - x_j||^2/2\sigma_i^2\right)}{\sum_{k\neq i}
exp\left(-||x_i - x_k||^2/2\sigma_i^2\right)},$$

$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2},$$

$$q_{ij} = \frac{\left(1 + ||\theta_i - \theta_j||^2\right)^{-1}}{\sum_{k\neq l}\left(1 + ||\theta_k - \theta_l||^2\right)^{-1}},$$

gdzie $\sigma_i \in \mathbb{R}^2$ dla $i = 1, \dots n$ to ustalone wcze¶niej parametry. Ustalmy funkcjê kosztu:

$$J_{\theta} = \sum_{i \neq j} p_{ij}log\frac{p_{ij}}{q_{ij}}.$$

Rozwi±zanie tego typu problemu mo¿emy uznaæ jako znalezienie odpowiedniej reprezentacji $\theta_i$ dla ka¿dego z punktów $x_i$, gdzie $i = 1, \dots, n$. Na poni¿szym obrazku prezentujemy przyk³adowe rozwi±zanie dla nastêpuj±cego zbioru :

$$\mathcal{X} = \left\{\left(\sin\alpha, \cos\alpha, sin\beta, \cos\beta\right): \alpha, \beta = 0, \frac{2\pi}{n}, \frac{4\pi}{n}, \dots, 2\pi\right\} \subset \mathbb{R}^4,$$

czyli czterowymiarowego zanurzenia torusa w wypadku gdy $m=3$, $n=50$ oraz 
$$\sigma_1, \sigma_2, \dots, \sigma_{2500} = 1.$$

\begin{center}
\includegraphics[scale = 0.5]{torus_tsne.png}
\end{center}

Na obrazku widaæ, ¿e \textit{p±czkowata} struktura torusa zosta³a uchwycona w trójwymiarowej reprezentacji zbioru $\mathcal{X}$.

\end{przyklad}

\subsection{Generalizacja oraz przeuczenie}

W dotychczasowych rozwa¿aniach przyjêli¶my, ¿e g³ównym celem uczenia jest odnalezienie modelu, który minimalizuje ustalon± przez nas funkcjê b³êdu. Jednak problem uczenia zawiera w sobie jeszcze jedn± p³aszczyznê. Zbiór posiadanych przez nas danych mo¿e siê rozszerzyæ o nowe punkty - i naturalnym jest oczekiwanie od modelu, ¿e funkcja b³êdu bêdzie przybiera³a porównywalne warto¶ci na rozszerzonym zbiorze. Tak± zdolno¶æ modelu nazywamy \textit{generalizacj±}.

Zjawisko przeciwne, tzn. stan w którym warto¶æ funkcji b³êdu wzrasta znacz±co po dodaniu nowych punktów nazywamy \textit{przeuczeniem}.

\begin{przyklad}
Jednym z ciekawych, ale ma³o praktycznych sposobów sprawdzenia czy nasz model mo¿e do¶wiadczyæ zjawiska przeuczenia jest analiza wariancji modelów metod± tzw. resamplingu. Intuicyjnie - je¶li model ma uchwyciæ faktyczn± strukturê danych - to dla ró¿nych zbiorów danych pochodz±cych z tego samego ¼ród³a - modele zbudowane wedle tych zbiorów powinny byæ podobne. Du¿a wariancja w¶ród modeli mo¿e natomiast wskazywaæ na to, ¿e nasz model mo¿e do¶wiadczaæ zjawiska przeuczenia. Poni¿ej prezentujemy przyk³ad takiego sprawdzenia. Oryginalne dane pochodz± z wielomianu 4-go stopnia, którego wykres zaprezentowany jest w prawym górnym rogu obrazka. W kolejnych oknach prezentujemy ró¿ne modele wielomianowe wytrenowane na delikatnie zaburzonych danych podstawowych.  

\begin{center}
\includegraphics[scale = 0.4]{overfitting.png}
\end{center}

Widzimy, ¿e w przypadku wielomianów stopnia pierwszego mamy do czynienia z ma³± wariancj± modeli - jednakowo¿ - modele te odleg³e s± od dobrej aproksymacji wyj¶ciowej funkcji. W przypadku aproksymacji wielomianami stopnia 4 - zarówno jako¶æ aproksymacji oraz jednorodno¶æ modeli stoj± na wysokim poziomie. W wypadku aproksymacji modelami stopnia 16 - widzimy, ¿e zarówno jako¶æ aproksymacji oraz jednorodno¶æ modeli uleg³y znacz±cemu obni¿eniu. Modele tej rodziny mog± cierpieæ z powodu zjawiska przeuczenia.

\end{przyklad}

\subsubsection{Zbiory walidacyjne i testowe}

Czêstym i naturalnym przeznaczeniem modelu jest jego zastosowanie na nowych danych. Dlatego sprawdzenie czy wyniki uzyskiwane na przyk³adach pochodz±cych spoza zbioru treningowego s± porównywalne do tych uzyskiwanych na zbiorze testowym w wielu przypadkach stanowi kluczowy punkt w ocenie przydatno¶ci rozwi±zania. W celu zasymulowania nap³ywu nowych danych stosuje siê tzw. technikê \textit{zbioru testowego}, w której czê¶æ danych nie uczestniczy w dzia³aniu algorytmu optymalizacyjnego - lecz mierzy aktualn± skuteczno¶æ modelu i pozwala np. zaobserwowaæ czy model dzia³a znacznie lepiej na przyk³adach treningowych ni¿ testowych (co jest zaprzeczeniem tego, ¿e model powinien dobrze radziæ sobie dla nowych danych).

Czêstym zjawiskiem jest tak¿e wyodrêbnianie jeszcze jednego zbioru - tzw. \textit{zbioru walidacyjnego}. Celem tego wyodrêbnienia jest unikniêcie sytuacji w której rozwijaj±cy rozwi±zanie wykorzystuj± informacjê ze zbioru testowego w trenowaniu. Wówczas nie jest uczciwym powiedzenie, ¿e zbiór testowy \textit{na¶laduje} nap³yw nowych danych. Dlatego ostateczny wynik potwiedzony jest na osobnym \textit{zbiorze walidacyjnym}, na którym rozwi±zanie u¿yte jest dok³adnie jeden raz i które wynik jest ostateczn± estymat± u¿yteczno¶ci rozwi±zania.

Warto podkre¶liæ, ¿e w niektórych ¼ród³ach okre¶la siê osobny zbiór wykorzystywany podczas trenowania mianem \textit{zbioru testowego}, a ostateczny wynik uzyskiwany jest na zbiorze okre¶lanym mianem \textit{zbioru walidacyjnego} (jest to notacja odwrotna do przedstawionej w tej pracy). 

\subsubsection{Walidacja krzy¿owa (ang. \textit{cross-validation})}

Wyra¿nymi wadami techniki \textit{zbioru walidacyjnego} s± dwa nastêpuj±ce fakty:

\begin{itemize}
\item tylko niewielki procent posiadanych danych zostaje wykorzystany do symulowania nap³yniêcia nowych danych, co czyni estymatê skuteczno¶ci modelu mniej dok³adn±,
\item wynik modelu mo¿e zostaæ wyra¿nie zaburzony je¶li wydzielony do testowania zbiór bêdzie mia³ dystrybucje znacz±co ró¿n± od dystrybucji zbioru treningowego (tzw. szum podzia³u - ang. \textit{bias split}).

\end{itemize}

Aby unikn±æ tych dwóch niedogodno¶ci stosujê siê tzw. metodê \textit{$k$-walidacji krzy¿owej}, w której zbiór treningowy dzielony jest na $k$ róznych czê¶ci (ang. \textit{folds}), a nastêpnie wybiera siê kolejne $k-1$ elementowe zbiory czê¶ci jako zbiory treningowe, a niewybran± czê¶æ wykorzystuje siê do walidacji. Uzyskane w ten sposób $k$ wyników wykorzystuje siê do ostatecznej estymaty (najczê¶ciej badaj±c ich rozk³ad i bior±c ¶redni± wyników jako ostateczny wynik).

Dzieki tej metodzie ka¿dy element zbioru treningowego uczestniczy w symulowaniu nap³ywu nowych danych, a zjawisko szumu podzia³u jest mniej szkodliwe, jako, ¿e wielokrotne jego powtórzenie jest zjawiskiem bardzo rzadkim. Wszystko to odbywa siê kosztem czasu uczenia - w zwi±zku z wielokrotnym uczeniem modelu technika ta wymaga zazwyczaj znacznie wiêcej czasu ni¿ uczenie wykorzystuj±cy podzia³ na zbiory \textit{treningowy-testowy-walidacyjny}.

Warto dodaæ - ¿e ze zbioru treningowe czêsto wydziela siê osobny zbiór testowy, który uczestniczy w procesie uczenia w sposób opisany w poprzedniej subsekcji.


\subsection{Optymalizacja gradientowa}

Jak wspominali¶my w podsekcji \ref{defucz}, algorytmy uczenia to de facto algorytmy rozwi±zuj±ce pewne zadanie optymalizacyjne. Aktualnie tylko dla niewielkiej czê¶ci z nich potrafimy odnale¼æ rozwi±zanie w sposób ¶ci¶le analityczny. Dlatego dla du¿ej grupy problemów uczenia maszynowego stosuje siê tzw. metody optymalizacji gradientowej. Schemat dzia³ania wiêkszo¶ci z tych metod przybli¿a poni¿szy algorytm (\cite{gradient-descents}).

\begin{algorithm}
  \KwData{Zbiór danych $\mathcal{X}$, rodzina modeli $\Theta$ parametryzowana przez argumenty $\theta_1, \theta_2, \dots, \theta_n$ oraz ró¿niczkowalna funkcja b³êdu $J_\mathcal{X}$.}
  \KwResult{Warto¶ci parametrów $\theta_1, \theta_2, \dots, \theta_n$ dla których rozwi±zanie jest mo¿liwie najlepsze ze wzglêdu na funkcjê kosztu $J_\mathcal{X}$.}
  zainicjuj wagi $\theta_1, \theta_2, \dots, \theta_n$ w sposób losowy. \;
  oblicz $J_{\mathcal{X}}\left(\theta_1, \theta_2, \dots, \theta_n\right)$\;  
  \While{$J_\mathcal{X}\left(\theta_1, \theta_2, \dots, \theta_n\right)$ nie spe³nia warunku stopu} {
    oblicz $\delta_i = \frac{\partial J_{\mathcal{X}}\left(\theta_1, \theta_2, \dots, \theta_n\right)}   {\partial \theta_i}$ dla ka¿dego $i = 1, \dots, n$\;    
    zaktualizuj: $\theta_{i} := \theta_i - \delta_i\eta$ dla ka¿dego $i = 1, \dots, n$\;
    ponownie oblicz $J_{\mathcal{X}}\left(\theta_1, \theta_2, \dots, \theta_n\right)$\;  
    } 
    \caption{Uproszczony schemat optymalizacji gradientowej}
\end{algorithm}

Oczywi¶cie powy¿szy schemat zosta³ uproszczony w celu uchwycenia g³ównej idei. Zgodnie z matematycznym faktem, który mówi, ¿e gradient wyznacza kierunek najwy¿szego wzrostu, pod±¿anie w kierunku przeciwnym do wyznaczonego przez niego jest to¿same pod±¿aniu w kierunku najwiêkszego spadku. 
Sta³a $\eta$, która pojawia siê w algorytmie nazywana jest \textit{sta³± uczenia} (ang. \textit{learning rate}) i powinna byæ dobierana ostro¿nie. W wielu algorytmach mo¿e ona zmieniaæ siê wraz z rosn±c± liczb± iteracji, a najskuteczniejsze aktualnie metody gradientowe ustalaj± j± nawet jako funkcjê wag oraz historii kolejnych iteracji (\cite{adam} \cite{nadam}).
Warunek stopu wystêpuj±cy w warunku pêtli natomiast najczê¶ciej ustalany jest jako zej¶cie warto¶ci funkcji kosztu $J_\mathcal{X}$ poni¿ej pewnej ustalonej warto¶ci $\epsilon$ lub brak dostatecznej poprawy (wzglêdnej lub bezwzglêdnej) w stosunku do warto¶ci obliczonej w poprzedniej iteracji. 

Innym czêsto spotykanym schematem jest ustalenie pewnej du¿ej ilo¶ci operacji uczenia (zwanych czêsto \textit{epokami} - ang. \textit{epochs}) i brania jako ostatecznego modelu albo rozwi±zania uzyskanego po ostatniej epoce, albo rozwi±zania które uzyska³o najlepszy wynik na \textit{zbiorze testowym}.

\subsubsection{Redukcja sta³ej uczenia w wypadku wysokiej i sta³ej warto¶ci funkcji kosztu (ang. \textit{learning rate reduction on plateau})} 

Kolejn± technik± wykorzystywan± podczas uczenia gradientowego jest tzw. \textit{redukcja sta³ej uczenia w wypadku sta³ej i wysokiej warto¶ci kosztu uczenia}. Opiszemy j± szerzej w zwi±zku z tym, ¿e zostanie wykorzystana w rozwi±zaniu opisanym w kolejnych rozdzia³ach tej pracy. Metoda ta, czêsto u¿ywana podczas uczenia gradientowego, zak³ada, ¿e gdy kolejne epoki nie przynosz± znacz±cej poprawy - oznacza to, ¿e lokalny kszta³t funkcji kosztu (w zale¿no¶ci od parametru) zawiera subtelno¶ci, które nie s± dostatecznie dobrze uwzglêdniane przez zbyt du¿± warto¶æ \textit{sta³ej uczenia}. Dlatego zwyk³o siê przemna¿aæ j± przez ustalon± sta³± $p < 1$ (czêsto przyjmuje siê $p=0.1$) w celu uwra¿liwienia metody optymalizacji na znacznie mniejsze, lokalne zmiany gradientu (\ref{r-o-p}).

\begin{figure}\label{r-o-p}
\centering 
\includegraphics[scale=0.9]{r-o-p.jpeg}
\caption{Przyk³ad zmiany funkcji koszty po redukcji sta³ej uczenia - za \cite{res-net}.}
\end{figure}


\subsection{Stochastyczna optymalizacja gradientowa}

Bardzo czêsto, w przypadku gdy liczno¶æ zbioru danych $\mathcal{X}$ jest wyj±tkowo du¿a, obliczenie funkcji b³êdu, a tak¿e jej gradientów stanowi zadanie nieefektywne obliczeniowo. W takich przypadkach czêsto stosowan± technik± jest metoda stochastycznej optymalizacji gradientowej. Najczê¶ciej polega ona na losowym podziale zbioru na mniejsze czê¶ci i sukcesywnym stosowaniu metody gradientowej na kolejnych elementach tego podzia³u. Zwyczajowo zbiory dzieli siê porcje (ang. \textit{batch}) równej liczno¶ci i ze wzglêdu na rozmiar tych porcji wyró¿niamy:
\begin{itemize}
\item uczenie \textit{online} w którym ka¿da porcja sk³ada siê z jednego przyk³adu,
\item uczenie \textit{mini-batch}, w którym liczno¶æ porcji jest znacz±co mniejsza od rozmiaru ca³ego zbioru,
\item uczenie \textit{full-batch}, w którym mamy tylko jedn± porcjê - sk³adaj±c± siê z ca³ego zbioru.
\end{itemize}

Co warte podkre¶lenia, losowanie tych danych wprowadza do uczenia dosyæ spor± dozê losowo¶ci. Mo¿e to oczywi¶cie znacz±co spowolniæ proces uczenia lub doprowadziæ do tego, ¿e odnaleziony przez nas model bêdzie daleki od optymalnego. Okazuje siê jednak, ¿e w praktyce odpowiedni dobór rozmiaru próbki, w po³±czeniu z zastosowaniem metod granicznych (\cite{minibatch}) przynosi znacz±ce przyspieszenie algorytmów ucz±cych przy minimalnej stracie poprawno¶ci. Intuicja która za tym stoi, mówi, ¿e pomimo i¿ dana porcja mo¿e delikatnie zaburzyæ nasz model, to ¶rednio (poniewa¿ \textit{de facto} próbkujemy nasz zbiór danych) uzyskamy krok w dobrym kierunku (\cite{loss-landscapes}).

\subsection{Metaoptymalizacja}

W poprzednie podsekcji przedstawili¶my sposób znalezienia optymalnego rozwi±zania w przypadku gdy zale¿no¶æ funkcji b³êdu od parametrów jest ró¿niczkowalna. Jednak czêsto w tego typu zagadnieniach mamy do czynienia tak¿e z parametrami które nie maj± tej w³a¶ciowo¶ci. Tradycyjnie nazywamy takie parametry \textit{metaparametrami}. W tego typu przypadkach najczê¶ciej sposób dzia³ania wygl±da nastêpuj±co: 

\begin{enumerate}
\item Podzielmy zbiór paramterów $\Theta$ na $\Theta_g$ i $\Theta_m$ gdzie $\Theta_g$ to parametry ze wzglêdu na które funkcja b³êdu jest ró¿niczkowalna, a $\Theta_m$ to \textit{metaparametry}. Zak³adamy, ¿e $\Theta = \Theta_g \times \Theta_m$, a tak¿e, ¿e $\Theta_m = \Theta^1_m \times \dots \times \Theta^k_m$ dla pewnego $k$ naturalnego. 
\item Dla ka¿dego $i = 1,\dots,k$ wybierzmy skoñczony podzbiór $\bar{\Theta}^i_m$. Oznaczmy przed $grid = \bar{\Theta}^1_m \times \dots \times \bar{\Theta}^k_m$.
\item Dla ka¿dego $g \in grid$ ustalmy kryterium oceny danego zbioru metaparametrów. Mo¿e to byæ np. warto¶æ funkcji b³êdu uzyskana przy pomocy gradientowej optymalizacji parametrów z rodziny $\Theta_g$.
\item Dla ka¿dej warto¶ci $g \in grid$ obliczmy warto¶æ kryterium opisanego w podpunkcie 3 i jako ostateczny wybierzmy model który minimalizuje to kryterium.
\end{enumerate}

Powy¿szy schemat prezentuje tzw. algorytm \textit{grid search}. Jednak w przypadku gdy warto¶æ $|grid|$ jest du¿a - obliczenia te mog± byæ nieosi±galne ze wzglêdu na naturê obliczeniow±. W takich przypadkach stosuje siê lekko zmodyfikowany schemat nazywany \textit{random search}. W schemacie tym ustalamy pewn± liczbê $k <= |grid|$, a nastêpnie losujemy $k$ krotnie bez zwracania warto¶ci z $grid$ wybieraj±c na koñcu najlepsz± warto¶æ spo¶ród wylosowanych parametrów. O zaskakuj±cej skuteczno¶ci tego podej¶cia mo¿na poczytaæ chocia¿by w \cite{randomsearch}.

\section{Sztuczne Sieci neuronowe}

W tym rozdziale zajmiemy siê omówieniem podstawowego narzêdzia, z jakiego korzystaj± wszystkie algorytmy g³êbokiego uczenia, czyli sztucznych sieci neuronowych (ang. \textit{artificial neural network}). Co ciekawe, w literaturze nie istnieje jedna, formalna definicja wspólna dla wszystkich sieci neuronowych. Dlatego w poni¿szym rozdziale, przeanalizujemy ró¿ne przyk³ady sieci (przygl±daj±c siê ich historycznemu rozwojowi) które przybli¿± nam podstawowy koncept ich funkcjonowania, a tak¿e zg³êbimy biologiczn± inspiracjê, która stoi za ich wynaleziem, czyli uk³ad nerwowy zwierz±t z jego ewolucyjnym zwiêczeniem (czyli ludzkim mózgiem).

\subsection{Biologiczna motywacja}

Badania naukowców i neurobiologów nad dzia³aniem ludzkiego mózgu trwaj± od nie z ma³a 100 lat i nadal skrywa on przed badaczami ogrom tajemnic. Jednak ju¿ dzisiaj mo¿emy z ca³± pewno¶ci± stwierdziæ, ¿e dzia³anie ludzkiego mózgu opiera siê na zorganizowanej wspó³pracy ogromnej ilo¶ci komórek nerwowych. Ka¿dy neuron przyjmuje od po³±czonych z nim innych neuronów nieregularnie w czasie aktywacje elektryczne i na ich podstawie sam emituje (lub nie) impuls elektrycznych do neuronów z którymi jest po³±czony. Zasada wspó³pracy ze sob± wielu jednostek obliczeniowych (jako tak± mo¿emy traktowaæ neuron) stanowi podstawê niemal ka¿dej architektury sieci neuronowych.

\subsection{Regresja logistyczna}

Jednym z najpopularniejszych algorytmów uczenia maszynowego jest aktualnie algorytm tzw. \textit{regresji logistycznej} (\cite{logistic-regression}). Z naszej strony jest on interesujacy poniewa¿ mo¿emy interpretowaæ go jako pojedynczy obliczeniowy neuron. Zacznijmy jednak od formalnej definicji:

\begin{defi}
Modelem regresji logistycznej parametryzowanej wektorem $\left(\theta_0, \theta_1, \dots, \theta_n\right)$ bêdziemy nazywali funkcjê $f : \mathbb{R}^n \rightarrow (0,1)$ zadan± wzorem:

$$f(x) = \phi\left(\theta_0 + \sum_{i = 1}^{n}\theta_i x_i\right),$$

gdzie $\phi$ to sigmoidalna funkcja aktywacji opisana w paragrafie \ref{sigmoid}.

\end{defi}

Widzimy zatem, ¿e je¶li zinterpretujemy $x_1, x_2, \dots x_n$ warto¶ci impulsów z komórek nerwowych s±siaduj±cych z nasz±, parametry $\theta_1, \theta_2, \dots, \theta_n$ jako si³y po³±czeñ naszego neuronu ze swoimi s±siadami oraz $\theta_0$ jako wewnêtrzn± pobudliwo¶æ neuronów - to widzimy, ¿e w oparciu o interakcjê z s±siadami - nasz neuron oblicza sw± wewnêtrzn± aktywacjê, przekazywan± jako wynik funkcji $f$.
\subsubsection{Sigmoidalna funkcja aktywacji} \label{sigmoid}

\begin{defi}
Sigmoidaln± (logistyczn±) funkcj± aktywacji bêdziemy nazywaæ funkcjê :
$$S(x) = \frac{1}{1 + e^{-x}}.$$
\end{defi}

Oto niektóre z jej wa¿nych w³asno¶ci:

\begin{uwaga}
Dla funkcji logistycznej zdefiniowanej powy¿ej zachodzi :
\begin{itemize}
\item $\lim_{x\rightarrow + \infty} S(x) = 1,$
\item $\lim_{x\rightarrow - \infty} S(x) = 0,$
\item $S'(x) = S(x)(1 - S(x)),$
\item $\lim_{x\rightarrow + \infty} S'(x) = 0$ \textit{saturacja prawostronna},
\item $\lim_{x\rightarrow - \infty} S'(x) = 0$ \textit{saturacja lewostronna}.
\end{itemize}
\end{uwaga}

\begin{figure}
\includegraphics[scale=0.7]{Sigmoid-wykres.png}
\caption{Wykres funkcji sigmoidalnej.}
\end{figure}

Zauwa¿my, ¿e na funkcjê sigmoidaln± mo¿emy spojrzeæ jako na ci±g³e przybli¿enie funkcji charakterystycznej zbioru $\{x\in\mathbb{R} : x > 0\}$. Kluczowe w zrozumieniu sekcji \cite{TODO} bêdzie natomiast w³asno¶æ saturacji. Zauwa¿my, ¿e dla liczb o du¿ej warto¶ci bezwzglêdnej gradient funkcji logistycznej praktycznie siê zeruje. Mo¿e to spowodowaæ znacz±ce spowolnienie uczenia w wypadku wyboru metody optymalizacji gradientowej.

\subsubsection{Logistyczna funkcja kosztu - (ang. \textit{log-loss})}

TODO

\subsubsection{Uogólnienie funkcji \textit{log-loss} do zagadnienia multiklasyfikacji}

TODO

\section{Klasyczne sieci neuronowe oraz algorytm wstecznej propagacji}

Regresja logistyczna - dziêki swojej prostocie - zyska³a sobie miano bardzo popularnej metody w dziedzienia uczenia maszynowego. Ogromnym problemem jaki napotykamy jednak przy jej u¿ywaniu jest to, ¿e dziêki swojej prostocie - nie jest w stanie efektywnie nauczyæ siê bardziej skomplikowanych funkcji. Szybko jednak zauwa¿ono, ¿e tak jak uk³ady nerwowe zwierz±t nie sk³adaj± sie tylko i wy³±cznie z jednego neuronu, tylko z ich wspó³dzia³aj±cego ze sob± konglomeratu, tak skutecznie dzia³aj±cych modeli nie nale¿y szukaæ w funkcjach na¶laduj±cych pojedyncze neurony, lecz w z³o¿eniu takich funkcji ze sob±. Tak narodzi³a siê idea \textit{sieci wielowarstwowych} które stanowi± podstawê tak¿e algorytmów \textit{deep-learning}.

\subsection{Topologie sieci}

Aby pozbyæ siê problemu jaki niesie ze sob± prostota perceptronu, przyjêto, ¿e wyj¶cie z wielu perceptronów, które jako argumenty przyjmuj± ten sam wektor $x$ czyli :

$$y^{(1)}_k = \phi_1\left(\sum_{i = 1}^{n} x_i w^{(1)}_{i, k} + w^{(1)}_{0, k}\right),$$

stanie siê automatycznie \textit{wej¶ciem} dla kolejnego perceptronu :

$$y^{(2)}_j = \phi_2\left(\sum_{k = 1}^{m} y_k w^{(2)}_{k, j} + w^{(2)}_{0, j}\right).$$

Oczywi¶cie proces tworzenia takiej sieci mo¿na iterowaæ i tak np. trzecia wartstwa mia³aby postaæ :

$$y^{(3)}_l = \phi_3\left(\sum_{j = 1}^{m} y_j w^{(3)}_{j, l} + w^{(3)}_{0, l}\right).$$

\begin{figure}\label{siec}
\centering 
\includegraphics[scale=0.7]{SiecXor.png}
\caption{Przyk³ad architektury wartstwowej sieci typu \textit{feed-forward}. Sieæ o tej architekturze jest w stanie nauczyæ siê logicznej funkcji XOR.}
\end{figure}

Dla uproszczenia przyjêli¶my, ¿e w obrêbie warstwy funkcji aktywacji s± takie same, jednak nie stanowi to koniecznego wymogu. Przedstawiona powy¿ej architektura stanowi przyk³ad architektury warstwowej - kolejne warstwy przyjmuj± jako swoje argumenty rezultaty obliczeñ z poprzednich warstw (st±d nazwa \textit{wielowarstwowy perceptron}). O wektorze wej¶ciowym $x$ zwyk³o siê zazwyczaj mówiæ jako o warstwie wej¶ciowej (ang.\textit{input layer}), natomiast o ostatniej wartstwie jako warstwie wyj¶ciowej (ang. \textit{output layer}). Oczywi¶cie istniej± tak¿e architektury w których dopuszczamy po³±czenia pomiêdzy ró¿nymi warstwami (ang. \textit{skip-connections} \cite{skip-connections}). Topologiê ka¿dej sieci zwykle przedstawia siê w postaci skierowanego grafu z³o¿eñ (patrz rysunek \ref{siec}). Je¶li w grafie tym nie ma cykli, sieæ tak± nazywamy sieci± \textit{w przód} (ang. \textit{feed-forward neural net}), w przeciwnym przypadku mamy do czynienia z sieci± rekurencyjn± (ang. \textit{recurent neural net}).

\begin{figure}
\centering 
\includegraphics[scale=0.7]{Recurrent.png}
\caption{Przyk³ad architektury rekurencyjnej. Zwróæmy uwagê na skierowany cykl w grafie po³±czeñ.}
\end{figure}

\subsection{Wielowarstwowa sieæ neuronowa jako uniwersalny aproksymator}

Opisane powy¿ej architektury nie s± oczywi¶cie jedyn± form± z³o¿enia ze sob± wielu perceptronów. £±cz± one jednak ze sob± dwie podstawowe wa¿ne cechy - prostotê parametryzacji zbiorem wag $w_{i}^{(l)}$ wraz z ogromn± zdolno¶ci± aproksymacji. O niesamowitej mo¿liwo¶ci aproksymacji przez sieci neuronowe mówi poni¿sze twierdzenie (dowód mo¿na znale¼æ np. tu \cite{cybenko}).

\begin{twierdzenie}
Niech $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ bêdzie dowoln± funkcj± ci±g³± o zwartym no¶niku. Dla ka¿dego $\epsilon > 0$ istnieje wówczas dwuwarstwowa sieæ neuronowa $f^* :  \mathbb{R}^{n} \rightarrow \mathbb{R}$ z sigmoidaln± funkcj± aktywacji dla której :
$$\sup_{x\in \mathrm{supp}f} \left|f(x) - f^*(x)\right|\leq \epsilon.$$
\end{twierdzenie}

Powy¿sze twierdzenie mówi nam o problemie regresji funkcji g³adkich. Analogiczne twierdzenie (\cite{hornik}) mówi nam, ¿e dowolny problem klasyfikacji da siê rozwi±zaæ z dowoln± dok³adno¶ci± przy pomocy trójwarstwowej sieci neuronowej. 

W³asno¶æ uniwersalnej aproksymacji daje nam gwarancjê, ¿e dla ka¿dego problemu, znajdziemy sieæ neuronow± która rozwi±zuje go z dowoln± dok³adno¶ci±. Nie nale¿y jednak przeceniaæ jej znaczenia - ich dowody nie przedstawiaj± nam ¿adnej skutecznej metody do znajdowania architektury oraz odpowiednich parametrów sieci.

\subsection{Algorytm wstecznej propagacji}

To, ¿e do³o¿enie kolejnych warstw zwiêksza mo¿liwo¶ci perceptronu wiedziano ju¿ w latach '60 XXw. To co przyczyni³o siê do prze³amania z³ego trendu, który nasta³ po rozczarowaniu zwi±zanym z ograniczeniami perceptronu by³ kolejny skuteczny algorytm uczenia. W tym przypadku kluczem sta³ siê algorytm wstecznej propagacji, dzia³aj±cy dla architektur typu \textit{feed-forward}. I ponownie - to co uderza, to ju¿ nie prostota samego algorytmu, lecz prostota koncepcji, która sta³a za jego wynalezieniem. To co okaza³o siê kluczowym w tym przypadku to idea znanej z rachunku ró¿niczkowego - czyli regu³a ³añcuchowa.

W swej najprostszej postaci algorytm ten jest kolejnym praktycznym zastosowaniem metody optymalizacji gradientowej. W sensie obliczeniowym, algorytm ten wykorzystuje brak cyklów w grafie obliczeñ, co znacz±co u³atwia obliczenia pochodnej b³êdu ze wzglêdu na ka¿dy parametr $w_{i}^{(j)}$. Spróbujmy prze¶ledziæ jak wygl±da obliczanie tych pochodnych, w przypadku gdy za funkcjê b³êdu przyjmiemy b³±d ¶redniokwadratowy, a za funkcjê aktywacji funkcjê sigmoidaln± dla sieci z jednym neuronem w wartstwie wyj¶ciowej. Przy zadanym zbiorze $\mathcal{X}$ warto¶æ pochodnej funkcji b³êdu ze wzglêdu na wyniki warstwy wyj¶ciowej $y_{x}^{(out)} = f^{*}(x)$, gdy dla ustalonego $x \in \mathcal{X}$ prawid³owa warto¶æ wynosi $y_x$ jest zadana wzorem :


\begin{figure}
\centering 
\includegraphics[scale=0.7]{Backprop.png}
\caption{Rysunek pomocniczy dla procesu wstecznej propagacji.}
\end{figure}

$$\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y^{(out)}} = \frac{\partial
\left(\frac{1}{2|\mathcal{X}|}\sum_{x \in \mathcal{X}} \left(y_{x}^{(out)} - y_{x}\right)^2\right)}{\partial y^{(out)}} = \sum_{x \in \mathcal{X}}\left(y_{x}^{(out)} - y_{x}\right).$$

Za³ó¿my teraz, ¿e dla ustalonej $k$ warstwy znamy pochodn± ze wzglêdu na ka¿de wyj¶cie $y_{i}^{(k)}$ czyli :

$$\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{i}^{(k)}}.$$

Przyjmuj±c wtedy za $x_i^{(k - 1)} = \sum_{j = 1}^{n_{k - 1}} w_{j, i}^{(k - 1)} y_{j}^{(k - 1)}$ Mamy wówczas (korzystaj±c z regu³y ³añcuchowej) :

$$\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial w_{i,j}^{(k - 1)}} = 
\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{i}^{(k)}}
\frac{\partial y_{i}^{(k)}}{\partial x_{i}^{(k - 1)}}\frac{\partial x_i^{(k - 1)}}{\partial w_{i,j}^{(k - 1)}}.$$

Powy¿szy wzór, korzystaj±c z w³asno¶ci funkcji sigmoidalnej upraszcza siê do :

$$\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial w_{i, j}^{(k - 1)}} = 
\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{i}^{(k)}}\left(1 - x_{i}^{(k - 1)}\right)x_{i}^{(k - 1)}y_{j}^{(k - 1)}.$$

Regu³a ³añcuchowa ponadto pozwala nam obliczyæ warto¶æ: 

\begin{equation}
\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{j}^{(k - 1)}} = 
\sum_{i = 1}^{n_{k - 1}}\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{j}^{(k)}}
\frac{\partial y_{j}^{(k)}}{\partial y_{i}^{(k - 1)}} =
\sum_{i = 1}^{n_{k - 1}}\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{j}^{(k)}}
\frac{\partial y_{j}^{(k)}}{\partial x_{i}^{(k - 1)}}
\frac{\partial x_{i}^{(k - 1)}}{\partial y_{j}^{(k - 1)}}.
\end{equation}

Po uproszczeniu (korzystaj±c z w³asno¶ci funkcji sigmoidalnej) otrzymujemy :

$$
\frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{j}^{(k - 1)}} = \frac{\partial J_{\mathcal{X}}\left(\Theta\right)}{\partial y_{i}^{(k - 1)}}\sum_{i = 1}^{n_{k - 1}}
\left(1 - x_{i}^{(k - 1)}\right)x_{i}^{(k - 1)}w_{i, j}^{(k - 1)}.$$

Warto¶ci te mo¿emy nastêpnie wykorzystaæ do obliczenia pochodnych ze wzglêdu na wagi z ni¿szych warstw. Zatem na mocy zasady indukcji matematycznej mo¿emy policzyæ gradient funkcji kosztu ze wzglêdu na dowolny parametr.

Zauwa¿my, ¿e w powy¿sze rozumowanie z ³atwo¶ci± mo¿na rozszerzyæ na przypadek sieci w których istniej± po³±czenia miêdzy warstwami. Wzory te - w ogólnej postaci, mo¿na równie¿ rozszerzyæ na inne przypadki funkcji b³êdu oraz aktywacji. 

Warto równie¿ podkre¶liæ, ¿e technika ta stosowana jest tak¿e w przypadku rekurencyjnych sieci neuronowych, przez odpowiednie przedstawienie sieci rekurencyjnej, jako sieci typu \textit{feed-forward} z dodatkowym kryterium, aby czê¶æ wag w sieci mia³y tak± sam± warto¶æ.

\subsubsection{Zjawisko przesuniêcia kowariancji (ang. \textit{covariate shift}) i technika normalizacji porcjowej (ang. \textit{Batch Normalization})}

Dosyæ powa¿n± wad± techniki wstecznej propagacji jest wystêpowanie tzw. zjawiska przesuniêcia kowariancji w sieciach typu \textit{feed-forward}. Intuicyjnie polega ono na tym, ¿e pod±¿anie w kierunku najmniejszego spadku w dwóch kolejnych warstwach mo¿e doprowadziæ do tego, ¿e pó¼niejsza z nich bêdzie oczekiwa³a kompletnie innych warto¶ci ni¿ te które dostarcza jej warstwa poprzednia. Czêsto znacznie pogarsza to stabilno¶æ oraz efektywno¶æ procesu uczenia.

Aby unikn±æ tego zjawisko czêsto stosuje siê tzw. \textit{technikê normalizacji porcjowej} (ang. \textit{Batch Normalization} \cite{batch-normalization})  gdzie ka¿da kolejna porcja danych u¿ywana w procesie uczenia - jest u¶redniana przez swoj± ¶redni± porcjow± $y^k_p$:

$$\bar{y}^{(k)} = y^{(k)} - y^{(k)}_{p},$$

standaryzowana przez porcjowe odchylenie standardowe $\sigma^k_p$:

$$\bar{\bar{y}}^{(k)} = \bar{y}^{(k)} - y^{(k)}_{p},$$ 

a nastêpnie przesuwana o wyuczalny wektor $m^{(k)}_p$  i skalowana przez wyuczalny czynnik $c^{(k)}_p$:

$$y^{(k)}_b = \bar{\bar{y}}^{(k)} * c^{(k)}_p + m^{(k)}_p.$$ 

Odbywa siê nie tylko na poziomie normalizacji danych wej¶ciowych, ale tak¿e pomiêdzy ka¿dymi kolejnymi warstwami. Ka¿de wej¶cie do kolejnej warstwy ma tak± sam± ¶redni± $m^{(k)}_p$ oraz odchylenie standardowe $c^{(k)}_p$ dziêki czemu zjawisko \textit{covariate shift} zostaje znacz±co ograniczone. W dniu dzisiejszym \textit{normalizacja porcjowa} uznawana jest za obowi±zkow± technikê normalizacji wiêkszo¶ci architektur sieci neuronowych. 


\subsection{Interpretacja probabilistyczna sieci neuronowych}

Dosyæ interesuj±cym spojrzeniem na zagadnienie poszukiwania optymalnych warto¶ci parametrów sieci neuronowej, jest przedstawienie tego zagadnienia optymalizacyjnego jako maksymalizowanie funkcji wiarygodno¶ci (dok³adnie jej logarytmu) modeli sposród ustalonej rodziny. Metodê tê, znan± ze statystyki jako estymacja metod± najwiêkszej wiarygodno¶ci (\textit{ang.} maximum likelihood estimation (MLE)). Rozwa¿my to poni¿szym przyk³adzie.

\begin{przyklad}
Rozwa¿my przez $f_{\theta}$ parametryzowan± wagami $\theta \in \Theta$ sieæ neuronow± o ustalonej architekturze typu $feed forward$. Rozwa¿my wówczas rodzinê rozk³adów warunkowych zadanych wzorem :
 
$$ \mathbb{P}\left(y | \theta, x\right) \sim \mathcal{N}\left(f_{\theta}(x), \sigma\right),$$

dla pewnej ustalonej liczby $\sigma > 0$. Mo¿emy to zinterpretowaæ w ten sposób, ¿e przy ustalonym $x$, rozk³ad $y$ przedstawia siê jako :

$$ y = f_{\theta}(x) + \mathbf{X},$$

gdzie $\mathbf{X} \sim \mathcal{N}\left(0, \sigma\right).$ Za³ó¿my teraz, ¿e nasz zbiór treningowy $\mathcal{X}$ wyznacza zbiór do zadania regresji i mo¿emy go przedtawiæ w postaci :
$$\mathcal{X} = \left\{\left(x_1, y_1\right), \left(x_2, y_2\right), \dots, \left(x_n, y_n\right)\right\} \subset \mathbb{R}^{n} \times \mathbb{R}.$$

Zauwa¿my, ¿e je¶li przyjmiemy, ¿e elementy naszego zbioru treningowego s± niezale¿ne oraz zgodne z powy¿szym rozk³adem prawdopodobieñstwa, to wówczas funkcja wiarygodno¶ci ustalonego zestawu wag $\theta \in \Theta$ wyra¿a siê wzorem:

$$\mathcal{L}\left(\theta, \mathcal{X}\right) = \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(y - f_{\theta}(x)\right)^{2}}{2\sigma}},$$

a zatem logarytm funkcji wiarygodno¶ci wyra¿a siê wzorem :

$$\log \mathcal{L}\left(\theta, \mathcal{X}\right) = -\frac{1}{2\sigma}\sum_{i = 1}^{n}\left(y - f_{\theta}(x)\right)^2 + C,$$

gdzie $C = -\frac{n}{2} \log 2\pi\sigma^{2}.$ Widzimy zatem, ¿e zadanie maksymalizacji logarytmu funkcji wiarygodno¶ci jest równowa¿ne rozwi±zaniu zadania minimalizacji ¶redniokwadratowej funkcji b³êdu dla rodziny funkcji neuronowych $f_\theta$, albowiem :

$$\arg \max_{\theta \in \Theta} -\frac{1}{2\sigma}\sum_{i = 1}^{n}\left(y - f_{\theta}(x)\right)^2 + C = \arg \min_{\theta \in \Theta}\frac{1}{2n}\sum_{i = 1}^{n}\left(y - f_{\theta}(x)\right)^2.$$

Analogia ta pozwala czêsto skorzystaæ z wygody dualnego patrzenia na zadanie uczenia - jako z jednej strony zagadnienia stricte optymalizacyjnego, a z drugiej - pokrewnego metod± statystycznym. Pozwala to skorzystaæ z wygodnych intuicji probabilistycznych, a tak¿e np. na ³atwe korzystanie z metod Bayesowskich w zagadnieniach machine learningu.

\end{przyklad}

Oczywi¶cie funkcja ¶redniokwadratowa nie jest jedyn± funkcj± b³êdu, dla której powy¿sza analogia ma miejsce. Je¶li np. zarz±damy aby :

$$ \mathbb{P}\left(y | \theta, x\right) \sim \mathcal{L}\left(f_{\theta}(x), 1\right),$$

gdzie $\mathcal{L}\left(f_{\theta}(x),\ 1\right)$ to rozk³ad Laplace'a z parametrami $f_{\theta}(x)$ oraz $1$, równowa¿n± metodzie MLE metod± optymalizacji bêdzie minimalizacja z nastêpuj±c± funkcj± b³êdu :

$$J_\mathcal{X}\left(\theta\right) = \sum_{x\in \mathcal{X}} \lVert f_{\theta}(x) - y\rVert_{1}.$$

Widzimy, ¿e rozumowanie to mo¿na uogólniæ niemal dla ka¿dej funkcji b³êdu, dla której istnieje taki rozk³ad prawdopodobieñstwa $p\left(\theta, x\right)$, ¿e 

$$ J_\mathcal{X}\left(\theta\right) = -C\sum_{x \in \mathcal{X}} \log p\left(x, \theta\right) + D,$$

gdzie $C, D \in \mathbb{R}$ to pewne sta³e. Przyk³ad zastosowania intuicji probabilistycznej mo¿emy zobaczyæ ju¿ w poni¿szym rozdziale.

\subsection{Regularyzacja sieci neuronowych}

W tej podsekcji chcia³bym opisaæ niektóre prostsze metody regularyzacji stosowane w sieciach neuronowych. Przez regularyzacjê rozumiemy dzia³ania maj±ce na celu zmniejszenie skutku procesu przeuczenia. W kolejnych rozdzia³ach poznamy inne metody regularyzacji (w tym te, które s± esencjonalne dla algorytmów \textit{deep learning}), w tym rozdziale jednak przedstawimy elementarne metody radzenia sobie ze zjawiskiem \textit{overfittingu}.

\subsubsection{Kara zwi±zana z $\lVert\Theta\rVert_{2}$}

Podstawow± metod± regularyzacji jest zmiana funkcji b³êdu poprzed dodanie dodatkowego sk³adnika, który zwiêksza warto¶æ b³êdu gdy norma Euklidesowa wektora wag jest du¿a. Prowadzi to do nowej funkcji b³êdu zadanej zazwyczaj wzorem:

$$J^{R}_\mathcal{X}(\Theta) = J_\mathcal{X}(\Theta) + \frac{\lambda}{2}\sum_{\theta \in \Theta}\theta^2$$

lub stosuj±c zapis wektorowy : 

$$J^{R}_\mathcal{X}(\Theta) = J_\mathcal{X}(\Theta) + \frac{\lambda}{2}\Theta\Theta^{T}.$$

dla pewnego $\lambda \in \mathbb{R}$ Warto zauwa¿yæ, ¿e stosuj±c probabilistyczn± interpretacjê sieci neuronowych, minimalizacja powy¿szej funkcji b³êdu jest równowa¿na maksymalizacji funkcji ufno¶ci przy za³o¿eniu rozk³adu \textit{a priori} na zbiorze parametrów $\Theta$:

$$\theta_{i} \sim \mathcal{N}\left(0 , \frac{1}{\lambda}\right), iid.$$

Za³ó¿my bowiem, ¿e istnieje rozk³ad $p \left(\theta, x\right)$ dla którego :

$$ J_\mathcal{X}\left(\theta\right) = -C\sum_{x \in \mathcal{X}} \log p\left(x, \theta\right) + D,$$

ale wówczas dok³adaj±c Bayesowskie za³o¿enie o warto¶ci parametrów $\theta$ otrzymujemy :

$$ J^{R}_\mathcal{X}\left(\theta\right) = -C\sum_{x \in \mathcal{X}} \log \left(\frac{p\left(x, \theta\right)}{p\left(\theta\right)} \right) + D = -C\sum_{x \in \mathcal{X}} \log \left(p\left(x, \theta\right)\right) + \frac{\lambda}{2}\sum_{\theta \in \Theta}\theta^2 + D',$$

co jest równowa¿ne powy¿szej formie funkcji kosztu.

\subsubsection{Kara zwi±zana z $\lVert\Theta\rVert_{1}$}

Inn± czêsto stosowan± metod± regularyzacji jest tzw. metoda \textit{lasso}, polegaj±ca na dodaniu do funkcji b³êdu dodatkowego sk³adnika zwiêkszaj±cego funkcjê b³êdu gdy tym razem pierwsza norma wektora wag jest du¿a. Funkcjê tê mo¿emy zapisaæ w postaci :

$$J^{R}_\mathcal{X}(\Theta) = J_\mathcal{X}(\Theta) + \sum_{\theta \in \Theta}|\theta| = 
J_\mathcal{X}(\Theta) + \lambda\lVert\Theta\rVert_{1}.$$

Analogicznie jak w poprzednim przypadku, mo¿emy potraktowaæ dodatkowy sk³adnik jako do³o¿enie Bayesowskiego za³o¿enia o rozk³adzie parametru wedle wzoru :

$$\theta \sim \mathcal{L}\left(0, \frac{1}{\lambda}\right),$$

gdzie przez $\mathcal{L}(a,b)$ oznaczamy rozk³ad Laplace'a z parametrami $a,b$.

\subsection{Czym ró¿ni± siê powy¿sze kary?}

Warto wspomnieæ o dosyæ powa¿nych koncepcyjnych ró¿nicach pomiêdzy powy¿szymi wagami. Przyjmuje siê (\cite{lasso}), ¿e o ile kara $\lVert\Theta\rVert_{2}$ stara siê niedopuszczaæ aby warto¶ci $\theta_i$ by³y wyra¼nie wiêksze od $0$ (funkcja kwadratowa ro¶nie szybko dla liczb $\gg\ 1$, o tyle kara $\lVert\Theta\rVert_{1}$ sprawia, ¿e znacznie czê¶ciej du¿a czê¶æ parametrów zbiór parametrów $\theta_i$ jest zaniedbywalnie wiêksza od $0$. Dlatego te¿ tê drug± metodê zwyk³o nazywaæ siê metod± \textit{lasso} i czêsto stosuje siê j± do eliminacji zbêdnych parametrów, poprzez odrzucenie tych dla których w wyniku uczenia warto¶ci otrzymane spe³nia³y $\theta \approx 0$.

\subsection{Intuicje dotycz±ce warstw sieci neuronowych}

To, ¿e trójwarstwowa sieæ neuronowa mo¿e poradziæ sobie z praktycznie ka¿dym zadaniem \textit{machine learningowym} jest faktem, udowodnionym teoretycznie, jednakowo¿ dowód ten jest niepraktyczny w tym sensie, ¿e dla zadanego zadania pokazuje istnienie rozwi±zania, bez dok³adnego przepisu na jego uzyskanie. Poni¿ej chcia³bym pokazaæ intuicyjny szkic dowodu tego, ¿e dowolny zbiór otwarty w $\mathbb{R}^n$ mo¿na przybli¿yæ przy pomocy sieci z dwiema warstwami ukrytymi :

\begin{enumerate}
\item Jednostki z sigmoidaln± funkcj± aktywacji, których argumentami s± neurony z warstwy wej¶ciowej, s± w stanie reprezentowaæ przybli¿one funkcje charakterystyczne pó³przestrzeni kowymiaru 1 (je¶li argumenty $x \in \mathbb{R}^n$.
\item Pojedynczy perceptron, z sigmoidaln± funkcj± aktywacji, jest w stanie w przybli¿ony sposób na¶ladowaæ operacje logiczne typu $\textsc{and}$ oraz $\textsc{or}$ je¶li swoje argumenty potraktuje jako przybli¿one warto¶ci \textit{boolowskie}.
\item W zwi±zku z powy¿szym, sieæ neuronowa z jedn± warstw± ukryt± i sigmoidaln± funkcj± aktywacji jest w stanie nauczyæ siê funkcji charakterystycznej dowolnego zbioru bêd±cego przeciêciem pó³przestrzeni (w szczególno¶ci - kostki $k-$wymiarowej, gdzie $k \leq n$).
\item Sieci neuronowe z dwiema warstwami ukrytymi potrafi± nauczyæ siê zatem przybli¿onej funkcji charakterystycznej dowolnej funkcji bêd±c± sum±, przeciêciem zbiorów których funkcje charakterystyczne mog± zostaæ uzyskane przy pomocy sieci z podpunktu 3. W szczególno¶ci - z dowolnym przybli¿eniem - dowolny zbiór otwarty $\mathcal{U} \subset \mathbb{R}^{n}.$ 
\end{enumerate}

Powy¿szy dowód uznajê za warto¶ciowy z dwóch powodów :

\begin{enumerate}
\item Pokazuje jak wielowarstwowa sieæ neuronowa koduje hierarchiczny zbiór cech, który nastêpnie wykorzystuje do budowania ostatecznego rozwi±zania.
\item W zwi±zku ze sw± ide± pokazuje, ¿e niekonstruktywne dowody w³asno¶ci uniwersalnej aproksymacji opieraj± siê li tylko na budowaniu lokalnych przybli¿eñ ostatecznego rozwi±znia, co koniec koñców mo¿e doprowadziæ do tzw. \textit{przekleñstwa wymiarowo¶ci} (ang. \textit{curse of dimensionality} \cite{curse-of-dimensionality}). 
\end{enumerate}

Oba powy¿sze wnioski rozwiniemy w kolejnych rozdzia³ach.

\subsection{Reprezentacja rozproszona}

Idea dzia³ania sieci opiera siê na budowaniu opisanej w poprzedniej podsekcji - struktury \textit{cech} potrzebnych do rozwi±zania zadanego problemu. Z jednej strony - stanowi to klucz do zrozumienia ogromnego sukcesu sieci neuronowych w ostatnim czasie. Fakt, ¿e jeden spójny algorytm uczenia mo¿e wyrêczyæ badacza z wyszukiwania cech niezbêdnych do rozwi±zania danego problemu (np. przy wspomnianym we wstêpie zagadnieniu rozpoznawania obrazu ekstrakcja takich cech by³a przez wiele lat ciê¿k± i dobrze p³atn± prac±), przerzucaj±c ciê¿ar pracy na zaprojektowanie odpowiedniej architektury sieci, funkcji kosztu dla procesu optymalizacji, a potem na moc obliczeniow± wspó³czesnych komputerów. Widzimy zatem, ¿e automatycznie budowany zbiór cech stanowi klucz do zrozumienia dzia³ania wielowarstwowych sieci neuronowych.

W klasycznych warstwowych sieciach neuronowych typu \textit{feed-forward} zbiór tych cech stanowi hierarchiczny konglomerat cech, w którym cechy reprezentowane w danej warstwie tworzone s± przez transfomacj cech reprezentowanych w warstwie poprzedniej (\ref{cnn-feat}, \cite{zeiler-fergus}). Jeden z przyk³adów takiej hierarchiczo¶ci przedstawi³em w szkicu dowodu aproksymacyjnego z poprzedniej podsekcji.  

Koncepcyjnie - zrezygnowanie z warstwowo¶ci, przy zachowaniu typu \textit{feed-forward} pozwala budowaæ nowe cechy przy pomocy konceptów wyuczonych przez ni¿sze warstwy. Dalsza relaksacja za³o¿eñ, czyli dopuszczenie cyklów w architekturze pozwala na budowanie cech oraz definicyjnych powi±zañ miêdzy nimi. W praktyce jednak, obliczeniowe w³asno¶ci architektury \textit{feed-forward} sprawiaj±, ¿e to w³a¶nie te sieci stanowi± wspó³cze¶nie awangardê w¶ród najskuteczniejszych rozwi±zañ zadañ \textit{machine learningu}. Nawet sieci rekurencyjne prezentuje siê z tej przyczyny, w tej formie, dodaj±c odpowiednie ograniczenia i warunki na warto¶ci odpowiednich wag (\cite{unfolding-rnn}).

\begin{figure}\label{cnn-feat}
\centering 
\includegraphics[scale=0.48]{zeilercnnfeatures.jpeg}
\caption{Wizualizacja cech wyuczonych przez tzw. konwolucyjn± sieæ neuronow±  z \cite{zeiler-fergus}.}
\end{figure}


\subsubsection{G³êboka hierarchiczna struktura przyczyn± sukcesu algorytmów z rodziny Deep Learning}

Przyjmuje siê, ¿e podstawow± przyczyn± ogromnego sukcesu g³êbokich sieci jest efektywna umiejêtno¶æ uchwycania hierarchicznych struktur wiedzy, która stanowi podstawê kompleksowo¶ci wielu z najwa¿niejszych wspó³cze¶nie problemów \textit{machine learning'owych} (\cite{bengio}). Prawid³owe zrozumienie rz±dz±cych tym prawide³ to nadal wielkie wyzwanie dla badaczy zajmuj±cych siê sztuczn± inteligencj±. Mo¿emy jednak na kilku przyk³adach przyjrzeæ siê dlaczego uchwycenie takiej struktury mo¿e okazaæ siê zbawienne dla efektywnego nauczenia modelu dla wielu wa¿nych zagadnieñ wspó³czesnego \textit{AI} (\cite{hierarchical}):

\begin{itemize}
\item Wiedza opisuj±ca np. rozpoznawanie twarzy ma hierarchiczn± strukturê. Podstawowe pojêcia takie jak kreski, ³uki, zabarwienia - s³u¿± do budowania bardziej z³o¿onych pojêæ takich jak oczy, policzki - które z kolei tworz± rozmaite rysy, twarze charakterystyczne dla rasy, p³ci, etc. 
\item Wiedza opisuj±ca rozpoznawanie mowy równie¿ ma podobn± strukturê - Podstawowe pojêcia tj. g³oski bior± udzia³ w tworzeniu fonemów, które sk³adaj± siê na s³owa tworz±ce zdania etc.
\end{itemize}

Przyjmuje siê, ¿e w³a¶nie taka postaæ wiedzy - naturalnie uchwycana przez model o hierarchicznej strukturze - stanowi podstawê sukcesów \textit{g³êbokich sieci}. W codziennej praktyce czêsto potwierdza siê - ¿e modele takie nie dzia³aj± tak efektywnie dla problemów o prostszej strukturze - chocia¿by ze wzglêdu na z³o¿ono¶æ przestrzeni parametrów, a tak¿e - z³o¿ono¶æ obliczeniow± procesu uczenia.

\chapter{Rozwój metod uczenia}

W swojej pracy przyj±³em konwencjê wedle której zamierzam przedstawiaæ szczegó³y technik \textit{deep learningowych} zgodnie chronologi± ich powstania. Uwa¿am tak± formê za odpowiedni±, poniewa¿ przedstawia je zgodnie z rosn±cym poziomem trudno¶ci oraz skomplikowania. Dlatego rozpoczniemy od pierwszej rewolucji, która pchnê³a sieci neuronowe w kierunku budowania bardziej realistycznej hierarchii cech - czyli wynalezienia sieci konwolucyjnych, a tak¿e omówienia pierwszej zaawansowanej i dzia³aj±cej sieci korzystaj±cej z tego paradygmatu czyli s³ynnej \textit{LeNet} autorstwa Yana LeCun'a.

 Kolejnym etapem bêdzie przedstawienie rewolucji zwi±zanej z now± form± treningu wstêpnego sieci, która przynios³a olbrzymi skok w jako¶ci uczenia. Przyjrzymy siê zatem powodom, dla których regularyzacja gwarantowana przez inicjacjê sieci jako autoenkoder lub Ograniczon± Maszynê Boltzmanna pozwoli³y przenie¶æ skuteczno¶æ dzia³ania sieci neuronowych na wy¿szy poziom.

Kolejne dwie sieci przenios± nas w ¶wiat nowoczesnych implementacji oraz skoku, który by³ zwi±zany z g³ównie ze wzrostem skuteczno¶ci obliczeñ, a zakoñczymy przegl±dem innych interesuj±cych sieci, których znajomo¶æ z pewno¶ci± poszerzy nasze intuicje dotycz±ce mo¿liwo¶ci jakie rozpo¶ciera przed badaczami \textit{deep learning}.  

\section{Sieci konwolucyjne oraz LeNet}

\subsection{Inwariancja}

W rozdziale opisuj±cym szczegó³y dzia³ania sieci neuronowych wspomniali¶my o tym, ¿e struktura sieci neuronowej wraz z wagami j± parametryzuj±cymi koduje pewn± ustalon± hierarchiê cech niezbêdnych do rozwi±zania postawionego przed ni± problemu. Powinny one zatem odwzorowywaæ wiêkszo¶æ w³asno¶ci, które posiadaj± rzeczywiste struktury rozwa¿anych problemów. Jedn± z takich w³asno¶ci, któr± posiada wiele zadañ do których rozwi±zania wykorzystujemy techniki \textit{deep learning} stanowi tzw. w³asno¶æ \textit{inwariancji}, czyli zamkniêto¶æ pewnej cechy na przekszta³canie argumentów przy pomocy ustalonej rodziny przekszta³ceñ. I tak np. cecha bycia okiem przez fragment obrazu lub zdjêcia jest zamkniêta na przekszta³cenia skalowania, przesuniêcia oraz obroty. Kwestia bycia d¼wiêkiem s³owa \textit{mama} jest zamkniêta na zmiany tonu g³osu, g³o¶no¶æ, a tak¿e umiejscowanie na ¶cie¿ce d¼wiêkowej. Przyk³ady mo¿na mno¿yæ - jednak zauwa¿my, ¿e w³asno¶æ inwariancji poci±ga za sob± dwie bardzo powa¿ne konsekwencje :

\begin{itemize}
\item Je¶li uda nam siê uchwyciæ tê inwariancje przy pomocy struktury sieci, mo¿emy zyskaæ ogromn± kompresje wiedzy, gdy¿ ogromna mnogo¶æ cech, spo¶ród których ka¿da powsta³a jako przekszta³cenie innej, bêdzie reprezentowana w sieci jako jedna.
\item W przypadku niemo¿no¶ci wychwycenia takiej inwariancji, skala trudno¶ci dramatycznie poniewa¿ :
\begin{itemize}
\item Ka¿da \textit{realizacja} danej abstakcyjnej cechy musi byæ kodowana w sieci osobno.
\item Dla ka¿dej z takich realizacji, musi istnieæ co najmniej jeden element w zbiorze treningowym, w którym one wystêpuje. 
\end{itemize}
\end{itemize}

Aby zrozumieæ skalê problemu, spróbujmy sobie wyobraziæ przypadek w którym chcieliby¶my nauczyæ sieæ neuronow± rozpoznawania oka na obrazku, w przypadku w którym nasza sieæ nie mog³aby realizowaæ w³asno¶ci \textit{inwariancji} ze wzglêdu na przesuniêcia na obrazku. W skrajnym przypadku musieliby¶my kodowaæ w³asno¶æ bycia ¶rodkiem oka dla ka¿dego piksela z osobno, co sprawia, ¿e problem jest o rz±d wielko¶ci trudniejszy, ni¿ w przypadku gdy mogliby¶my cechê bycia okiem zakodowaæ raz i \textit{zamkn±æ} nasz± strukturê ze wzglêdu na przekszta³cenie translacji.

\subsection{Sieci konwolucyjne}

Sieci konwolucyjne to opisana w \cite{conv} i rozwiniêta przez Yana LeCuna oraz Geoffrey'a Hintona architektura, która pozwala uchwyciæ inawariancjê ze wzglêdu na translacje w obrêbie wektora wej¶æ. Matematyczna struktura stoj±ca za sieciami konwolucyjnymi jest banalnie prosta. Oznaczmy przez $P_{i_{1}, i_{2}, \dots. i_{k}} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{k}$, gdzie $n \geq k$ operator rzutowania, tzn. 

$$P_{i_{1}, i_{2}, \dots. i_{k}}\left(x_{1}, x_{2}, \dots, x_{n}\right) = \left(x_{i_1}, x_{i_2}, \dots, x_{i_k}\right).$$

Przez \textit{warstwê konwolucyjn±} z wagami $w_{0}, w_{1}, \dots, w_{n}$, funkcj± aktywacji $\phi$ i rzutowaniami:

$$\mathsf{P} = \left(\left\{i^{(1)}_{1}, i^{(1)}_{2}, \dots, i^{(1)}_{k}\right\}, \left\{i^{(2)}_{1}, i^{(2)}_{2}, \dots, i^{(2)}_{k}\right\}, \dots, \left\{i^{(l)}_{1}, i^{(l)}_{2}, \dots, i^{(l)}_{k}\right\}\right).$$ 

bêdziemy rozumieæ funkcjê $f^{conv}_{\mathsf{P}} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{k},$ dla której $i-$ta wspó³rzêdna wektora $\left[f^{conv}_{\mathsf{P}}\left(x_{1}, x_{2}, \dots. x_{n}\right)\right]_{i}$ zadana jest wzorem :

$$\left[f^{conv}_{\mathsf{P}}\left((x_{1}, x_{2}, \dots. x_{n}\right)\right]_{i} = 
\phi\left(w_{0} + \sum_{j = 1}^{k} w_{i}x_{i_{j}} \right).$$

Zwróæmy uwagê, ¿e powy¿sz± warstwê mo¿na zinterpretowaæ jako warstwê w której ka¿dy neuron po³±czony jest tylko i wy³±cznie z pewnym $k$-elementowym podzbiorem indeksów argumentu, a tak¿e ¿e wszystkie neurony wspó³dziel± wagi miêdzy sob±. Pozwala to na inwariancje w tym sensie, ¿e wagi $w_0, w_1, \dots, w_n$ uczestnicz± w wykrywaniu pewnej cechy w kodowanych przez rzutowania regionach obrazu. 

Przedstawiona przez nas definicja konwolucyjnych sieci neuronowych jest bardzo ogólna. W szczególno¶ci nie do koñca jasnym mo¿e byæ sk±d wziê³o siê okre¶lenie sieci jako \textit{konwolucyjnej}. Pewnym wyt³umaczeniem mog± byæ poni¿sze przyk³ady obrazuj±ce dwa historyczne przyk³ady pierwszych topologii konwolucyjnych.

\begin{figure}
\centering 
\includegraphics[scale=0.7]{2d_conv.png}
\caption{Przyk³ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo¿na interpretowaæ jako z³o¿enie splotu wag $w_1,\dots,w_n$ z warto¶ciami z dwuwymiarowej macierzy oraz funkcji aktywacji.}
\end{figure}

\begin{figure}
\centering 
\includegraphics[scale=0.7]{1d_conv.png}
\caption{Przyk³ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo¿na interpretowaæ jako z³o¿enie splotu wag $w_1,\dots,w_n$ z warto¶ciami z wektora oraz funkcji aktywacji.}
\end{figure}

Oczywi¶cie warstwy konwolucyjne mo¿na ze sob± sk³adaæ, jedna po drugiej. Prowadzi to do uczenia inwariantnych ze wzglêdu na translacje cech z innych inwariantnych na przesuniêcia konceptów nauczonych przez ni¿sze warstwy. 

\subsection{Pooling}

Zauwa¿my, ¿e jedna warstwa sieci neuronowej mo¿e sk³adaæ siê z wielu warstw konwolucyjnych. Intuicyjnie odpowiada to uczeniu przez sieæ wielu rodzajów inwariantnych cech na danym poziomie. Jednak je¶li dla ka¿dej z tych warstw, moc zbioru rzutowañ bêdzie du¿a, mo¿e to doprowadziæ do powstania warstwy z ogromn± ilo¶ci± neuronów. Aby tego unikn±æ postanowiono automatycznie rozszerzyæ ka¿d± warstwê konwolucyjn± o automatycznie z³o¿on± z ni± kolejn±, zazwyczaj nieparametryczn±, warstwê konwolucyjn± o znaczenia mniejszej ilo¶ci neuronów, a tak¿e posiadaj±c± tê w³asno¶æ, ¿e rzutowania j± generuj±ce s± parami roz³±czne, zwan± warstw± \textit{poolingu}. Pozwala to znacznie zmniejszyæ ilo¶æ informacji trzymanej w pamiêci (obliczenia mo¿na dystrybuowaæ tak, aby w danym momencie trzymaæ informacjê tylko z rzutowañ po³±czonych z ustalon± jednostk± z warstwy \textit{poolingu}), a tak¿e zmniejsza ilo¶æ neuronów przekazywanych do kolejnej warstwie.

\subsection{LeNet}

Pierwsz± 	

\section{Autoenkodery, sieæ Hintona i Salakhudinowa}

\section{Sieæ Kryzhevskiego}

\section{GoogLeNet}

\section{Inne sieci}

\chapter{Moja implementacja}

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliografia}

\bibitem{twarze}
DeepFace: Closing the Gap to Human-Level Performance in Face Verification,
Yaniv Taigman, et al.
Conference on Computer Vision and Pattern Recognition (CVPR) 24 czerwca 2014.

\bibitem{znaki}
Recognizing Traffic Signs Using a Practical Deep Neural Network,
Hamed H. Aghdam, et al.
Robot 2015: Second Iberian Robotics Conference, 2 grudnia 2015.

\bibitem{mnist}
Gradient-based learning applied to document recognition,
LeCun et al.,
Proceedings of the IEEE, 86(11):2278-2324, November 1998.

\bibitem{perceptron}
The Perceptron: A probabilistic model for
information storage and organization
in the brain,
Frank Rosenblatt, 
Psychological Review, Vol.65, No. 6, 1958

\bibitem{mccul-pitts}
A logical calculus of the ideas immament in nervous activity,
Warren S.McCulloch and Walter Pitts,
Bulletin of Mathematical Biophysics, vol.5 1943

\bibitem{minsky-papert}
Perceptrons: An Introduction to Computational Geometry,
Marvin Minsky i Seymour Papert,
The MIT Press, Cambridge MA

\bibitem{backprop}
Learning representations by back-propagating errors,
David E. Rumelhart et al.,
Nature, 323	(6088) Pa¼dziernik 1986

\bibitem{conv}
Decomposition of surface EMG signals into single fiber action potentials by means of neural network,
Daniel Graupe, et al.,
Proc. IEEE International Symp. on Circuits and Systems, 1989

\bibitem{popper}
Logika odkrycia naukowego,
Karl Popper,
PWN, 1977

\bibitem{recurrent}
Generalization of backpropagation with application to a recurrent gas market model,
P. J. Werbos,
Neural Networks, 1, 1988.

\bibitem{cybenko}
Approximations by superpositions of sigmoidal functions,
G. Cybenko,
Mathematics of Control, Signals, and Systems 2 (4) (1988), 303-314

\bibitem{hornik}
Approximation Capabilities of Multilayer Feedforward Networks,Kurt Hornik (1991)
K. Hornik,
Neural Networks, 4 (2) (1991), 251?257

\bibitem{adam}
A Method for Stochastic Optimization,
D.P. Kingma, J. Ba,
CoRR, 2014 

\bibitem{minibatch}
Efficient mini-batch training for stochastic optimization,
Mu Li, et al.,
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 661-670 

\bibitem{randomsearch}
Random Search for Hyper-Parameter Optimization,
J. Bergstra, Y. Bengio,
Journal of Machine Learning Research 13 (2012) 281-305

\bibitem{lasso}
Regression Shrinkage and Selection via the lasso,
R. Tibshirani,
Journal of the Royal Statistical Society. Series B (methodological) 58 (1). Wiley: 267?88

\bibitem{bengio}
Learning Deep Architectures for AI,
Y. Bengio,
Foundations and Trends in Machine Learning Vol. 2, No. 1 (2009) 1?127

\bibitem{sentiment}
Domain Adaptation for Large-Scale Sentiment Classification:
A Deep Learning Approach,
X. Glorot et al.,
Proceedings  of  the  28th International  Conference  on  Machine  Learning,  Bellevue,  WA,  USA,  2011.

\bibitem{lem}
Dialogi,
S. Lem,
Wydawnictwo Literackie 1957 

\bibitem{vapnik-bet}
https://www.wired.com/2014/08/deep-learning-yann-lecun/

\bibitem{manifold-learning}
Manifold Learning: The Price of Normalization,
Y. Goldberg et al.,
Journal of Machine Learning Research 9 (2008) 1909-1939

\bibitem{gradient-descents}
An overview of gradient descent optimization algorithms,
http://ruder.io/optimizing-gradient-descent/

\bibitem{nadam}
Incorporating Nesterov Momentum into Adam,
T. Dozat,
Stanford CS 229 Projects - 2015

\bibitem{loss-landscapes}
Towards Understanding Generalization of Deep
Learning: Perspective of Loss Landscapes,
L. Wu et al.,
https://arxiv.org/pdf/1706.10239.pdf

\bibitem{logistic-regression}
The regression analysis of binary sequences (with discussion),
Cox, DR,
J Roy Stat Soc B. 20: 215 - 242 1958

\bibitem{skip-connections}
Skip Connections Eliminate Singularities,
A. E. Orhan, Xaq Pitkow,
arXiv:1701.09175

\bibitem{curse-of-dimensionality}
Nearest Neighbour Searches and the Curse of Dimensionality,
R. B. Marimont M. B. Shapiro,
IMA Journal of Applied Mathematics, Volume 24, Issue 1, 1 August 1979, Pages 59-70

\bibitem{unfolding-rnn}
Generalization of Back-Propagation to Recurrent Neural Networks,
F. J. Pineda,

\bibitem{zeiler-fergus}
Visualizing and Understanding Convolutional Networks,
M. D. Zeiler, R. Fergus,
arXiv:1311.2901

\bibitem{hierarchical}
Learning hierarchical categories in deep neural networks,
A. M. Saxe et al.,
Proceedings of the 35th annual meeting of the Cognitive Science Society. (pp. 1271-1276), 2013

\bibitem{res-net}
Deep Residual Learning for Image Recognition,
K. He et al.,
arXiv:1512.03385 

\bibitem{batch-normalization}
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
S. Ioffe, Ch. Szegedy,
arXiv:1502.03167 

\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
