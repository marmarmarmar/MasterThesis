\relax 
\catcode `"\active 
\select@language{polish}
\@writefile{toc}{\select@language{polish}}
\@writefile{lof}{\select@language{polish}}
\@writefile{lot}{\select@language{polish}}
\citation{twarze}
\citation{znaki}
\citation{sentiment}
\citation{mnist}
\citation{perceptron}
\citation{mccul-pitts}
\citation{minsky-papert}
\citation{minsky-papert}
\citation{lem}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Historia}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Pocz\k atki i perceptron}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Zima sztucznej inteligencji}{7}}
\citation{backprop}
\citation{backprop}
\citation{recurrent}
\citation{vapnik-bet}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Dlaczego perceptron zawi\'od\IeC {\l }?}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Algorytm wstecznej propagacji - odrodzenie i ponowna zima}{8}}
\citation{cybenko}
\citation{hornik}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Dlaczego sieci ponownie zawiod\IeC {\l }y?}{9}}
\citation{popper}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Matematyczne podstawy algorytm\'ow}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{matpod}{{2}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Uczenie}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formalna definicja uczenia}{11}}
\newlabel{defucz}{{2.1.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Uczenie z nadzorem}{12}}
\citation{manifold-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Uczenie bez nadzoru}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Generalizacja oraz przeuczenie}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Zbiory walidacyjne i testowe}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Walidacja krzy\.zowa (ang. \textit  {cross-validation})}{15}}
\citation{gradient-descents}
\citation{adam}
\citation{nadam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Optymalizacja gradientowa}{16}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Uproszczony schemat optymalizacji gradientowej}}{16}}
\citation{res-net}
\citation{minibatch}
\citation{loss-landscapes}
\newlabel{r-o-p}{{2.1.5}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Przyk\IeC {\l }ad zmiany funkcji koszty po redukcji sta\IeC {\l }ej uczenia - za \cite  {res-net}.}}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Redukcja sta\IeC {\l }ej uczenia w wypadku wysokiej i sta\IeC {\l }ej warto\'sci funkcji kosztu (ang. \textit  {learning rate reduction on plateau})}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Stochastyczna optymalizacja gradientowa}{17}}
\citation{randomsearch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Metaoptymalizacja}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sztuczne Sieci neuronowe}{18}}
\citation{logistic-regression}
\citation{TODO}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Biologiczna motywacja}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Regresja logistyczna}{19}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoidalna funkcja aktywacji}{19}}
\newlabel{sigmoid}{{2.2.2}{19}}
\citation{kullback-leibler}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Wykres funkcji sigmoidalnej.}}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Logistyczna funkcja kosztu - (ang. \textit  {log-loss})}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Uog\'olnienie funkcji \textit  {log-loss} do zagadnienia multiklasyfikacji}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Uog\'olnienie powy\.zszych funkcji kosztu do dowolnego modelu}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Klasyczne sieci neuronowe oraz algorytm wstecznej propagacji}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Topologie sieci}{21}}
\citation{skip-connections}
\citation{cybenko}
\newlabel{siec}{{2.3.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Przyk\IeC {\l }ad architektury wartstwowej sieci typu \textit  {feed-forward}. Sie\'c o tej architekturze jest w stanie nauczy\'c si\k e logicznej funkcji XOR.}}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Przyk\IeC {\l }ad architektury rekurencyjnej. Zwr\'o\'cmy uwag\k e na skierowany cykl w grafie po\IeC {\l }\k acze\'n.}}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Wielowarstwowa sie\'c neuronowa jako uniwersalny aproksymator}{22}}
\citation{hornik}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Algorytm wstecznej propagacji}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Rysunek pomocniczy dla procesu wstecznej propagacji.}}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Zjawisko przesuni\k ecia kowariancji (ang. \textit  {covariate shift}) i technika normalizacji porcjowej (ang. \textit  {Batch Normalization})}{24}}
\citation{batch-normalization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Interpretacja probabilistyczna sieci neuronowych}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Regularyzacja sieci neuronowych}{26}}
\citation{lasso}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{2}$}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{1}$}{27}}
\citation{curse-of-dimensionality}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Czym r\'o\.zni\k a si\k e powy\.zsze kary?}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Intuicje dotycz\k ace warstw sieci neuronowych}{28}}
\citation{zeiler-fergus}
\citation{unfolding-rnn}
\citation{zeiler-fergus}
\citation{bengio}
\citation{hierarchical}
\newlabel{cnn-feat}{{2.3.8}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Wizualizacja cech wyuczonych przez tzw. konwolucyjn\k a sie\'c neuronow\k a z \cite  {zeiler-fergus}.}}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}Reprezentacja rozproszona}{29}}
\@writefile{toc}{\contentsline {subsubsection}{G\IeC {\l }\k eboka hierarchiczna struktura przyczyn\k a sukcesu algorytm\'ow z rodziny Deep Learning}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Sieci konwolucyjne}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Inwariancja}{31}}
\citation{conv}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Przyk\IeC {\l }ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z dwuwymiarowej macierzy oraz funkcji aktywacji.}}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Architektura sieci konwolucyjnych}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Przyk\IeC {\l }ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z wektora oraz funkcji aktywacji.}}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Najpopularniejsze rzutowania}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Przyk\IeC {\l }ad obrazuj\k acy dzia\IeC {\l }anie filtr\'ow konwolucyjnych.}}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pooling}{34}}
\citation{receptive-field}
\citation{backprop}
\citation{lenet}
\citation{alex-net}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Przyk\IeC {\l }ad obrazuj\k acy dzia\IeC {\l }anie \textit  {poolingu}.}}{35}}
\@writefile{toc}{\contentsline {subsubsection}{Pooling globalny (ang. \textit  {global pooling})}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Przegl\k ad najpopularniejszych architektur konwolucyjnych}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Pierwsze sieci konwolucyjne}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}AlexNet}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Schemat architektury sieci \textit  {LeNet-5}.}}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Schemat architektury sieci \textit  {AlexNet}.}}{36}}
\@writefile{toc}{\contentsline {subsubsection}{Recitified Linear unit - ReLU}{36}}
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Wykres funkcji i s\IeC {\l }abej pochodnej \textit  {ReLU}.}}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Wykres funkcji \textit  {Leaky ReLU}.}}{37}}
\@writefile{toc}{\contentsline {subsubsection}{Obliczenia na kartach graficznych}{37}}
\citation{dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Przyspieszanie kart graficznych oraz zwi\k ekszanie r\'o\.znicy pomi\k edzy obliczeniami wykonywanymi na GPU / CPU.}}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Augmentacja obraz\'ow}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{38}}
\citation{vgg}
\citation{imagenet}
\citation{inception}
\citation{inception-movie}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Przyk\IeC {\l }ady obrazk\'ow uzyskanych z pojedynczego obrazka ze zbioru treningowego przy pomocy losowych augmentacji.}}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}VGG}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}GoogLeNet}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Przyk\IeC {\l }ady architektury sieci w kt\'orej zastosowane technik\k e \textit  {dropout}.}}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Schemat architektury sieci \textit  {VGG-19}.}}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Schemat architektury sieci \textit  {Google Inception v1}}}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\IeC {\L }odyga (ang. \textit  {stem}) sieci}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Konwolucja $(f_w, f_h) = (1, 1)$ - redukcja wymiarowo\'sci}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Klasyfikatory pomocnicze}{41}}
\citation{batch-normalization}
\citation{inception-v3}
\citation{inception-v4}
\citation{resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Przyk\IeC {\l }ad bloku incepcyjnego - \textit  {sieci w sieci}.}}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Blok incepcyjny - \textit  {sie\'c w sieci}}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Kolejne wersji Incepcji}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Sieci residualne ResNet}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Po\IeC {\l }\k aczenia residualne}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Przyk\IeC {\l }ad po\IeC {\l }\k aczenia residualnego.}}{43}}
\citation{nature-cancer}
\citation{nature-cancer}
\citation{wiki-cancer}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementacja - zastosowanie CNN do rozpoznawania raka jelita grubego}{45}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Opis problemu}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Opis zbioru danych i dotychczasowych wynik\'ow}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Zbi\'or danych}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Otrzymane wyniki}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Opis eksperyment\'ow i sieci}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Por\'ownanie wynik\'ow}{45}}
\bibcite{twarze}{1}
\bibcite{znaki}{2}
\bibcite{mnist}{3}
\bibcite{perceptron}{4}
\bibcite{mccul-pitts}{5}
\bibcite{minsky-papert}{6}
\bibcite{backprop}{7}
\bibcite{conv}{8}
\bibcite{popper}{9}
\bibcite{recurrent}{10}
\bibcite{cybenko}{11}
\bibcite{hornik}{12}
\bibcite{adam}{13}
\bibcite{minibatch}{14}
\bibcite{randomsearch}{15}
\bibcite{lasso}{16}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{47}}
\bibcite{bengio}{17}
\bibcite{sentiment}{18}
\bibcite{lem}{19}
\bibcite{vapnik-bet}{20}
\bibcite{manifold-learning}{21}
\bibcite{gradient-descents}{22}
\bibcite{nadam}{23}
\bibcite{loss-landscapes}{24}
\bibcite{logistic-regression}{25}
\bibcite{skip-connections}{26}
\bibcite{curse-of-dimensionality}{27}
\bibcite{unfolding-rnn}{28}
\bibcite{zeiler-fergus}{29}
\bibcite{hierarchical}{30}
\bibcite{res-net}{31}
\bibcite{batch-normalization}{32}
\bibcite{kullback-leibler}{33}
\bibcite{lenet}{34}
\bibcite{receptive-field}{35}
\bibcite{alex-net}{36}
\bibcite{imagenet}{37}
\bibcite{vgg}{38}
\bibcite{inception}{39}
\bibcite{inception-movie}{40}
\bibcite{inception-v3}{41}
\bibcite{inception-v4}{42}
\bibcite{resnet}{43}
\bibcite{dropout}{44}
\bibcite{nature-cancer}{45}
\bibcite{wiki-cancer}{46}
