\relax 
\catcode `"\active 
\select@language{polish}
\@writefile{toc}{\select@language{polish}}
\@writefile{lof}{\select@language{polish}}
\@writefile{lot}{\select@language{polish}}
\citation{twarze}
\citation{znaki}
\citation{sentiment}
\citation{mnist}
\citation{perceptron}
\citation{mccul-pitts}
\citation{minsky-papert}
\citation{minsky-papert}
\citation{lem}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Historia}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Pocz\k atki i perceptron}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Zima sztucznej inteligencji}{7}}
\citation{backprop}
\citation{backprop}
\citation{recurrent}
\citation{vapnik-bet}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Dlaczego perceptron zawi\'od\IeC {\l }?}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Algorytm wstecznej propagacji - odrodzenie i ponowna zima}{8}}
\citation{cybenko}
\citation{hornik}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Dlaczego sieci ponownie zawiod\IeC {\l }y?}{9}}
\citation{popper}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Matematyczne podstawy algorytm\'ow}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{matpod}{{2}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Uczenie}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formalna definicja uczenia}{11}}
\newlabel{defucz}{{2.1.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Uczenie z nadzorem}{12}}
\newlabel{mse}{{2.1.1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Uczenie bez nadzoru}{12}}
\citation{manifold-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Generalizacja oraz przeuczenie}{13}}
\newlabel{torus-tsne}{{2.1.2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Na obrazku wida\'c, \.ze \textit  {p\k aczkowata} struktura torusa zosta\IeC {\l }a uchwycona w tr\'ojwymiarowej reprezentacji zbioru $\mathcal  {X}$. Potwierdza to efektywn\k a redukcj\k e wymiarowo\'sci gdy\.z 4-wymiarowe zanurzenie torusa zosta\IeC {\l }o efektywnie przedstawione w $\mathbb  {R}^3$.}}{14}}
\citation{gradient-descents}
\@writefile{toc}{\contentsline {subsubsection}{Zbiory walidacyjne i testowe}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Walidacja krzy\.zowa (ang. \textit  {cross-validation})}{15}}
\citation{adam}
\citation{nadam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Optymalizacja gradientowa}{16}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Uproszczony schemat optymalizacji gradientowej}}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Losowa inicjalizacja}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Lokalne minima}{16}}
\citation{res-net}
\newlabel{r-o-p}{{2.1.5}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Przyk\IeC {\l }ad zmiany funkcji koszty po redukcji sta\IeC {\l }ej uczenia - za \cite  {res-net}.}}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Redukcja sta\IeC {\l }ej uczenia w wypadku wysokiej i sta\IeC {\l }ej warto\'sci funkcji kosztu (ang. \textit  {learning rate reduction on plateau})}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{17}}
\citation{minibatch}
\citation{loss-landscapes}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Stochastyczna optymalizacja gradientowa}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Metaoptymalizacja}{18}}
\citation{randomsearch}
\citation{logistic-regression}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sztuczne Sieci neuronowe}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Biologiczna motywacja}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Regresja logistyczna}{19}}
\newlabel{sigmo}{{2.2.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Wykres funkcji sigmoidalnej.}}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoidalna funkcja aktywacji}{20}}
\newlabel{sigmoid}{{2.2.2}{20}}
\citation{kullback-leibler}
\@writefile{toc}{\contentsline {subsubsection}{Logistyczna funkcja kosztu - (ang. \textit  {log-loss})}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Uog\'olnienie funkcji \textit  {log-loss} do zagadnienia multiklasyfikacji}{21}}
\newlabel{categorical-cross-entropy}{{2.2.2}{21}}
\citation{skip-connections}
\@writefile{toc}{\contentsline {subsubsection}{Uog\'olnienie powy\.zszych funkcji kosztu do dowolnego modelu}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Klasyczne sieci neuronowe oraz algorytm wstecznej propagacji}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Topologie sieci}{22}}
\citation{cybenko}
\newlabel{siec}{{2.3.1}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Przyk\IeC {\l }ad architektury wartstwowej sieci typu \textit  {feed-forward}. Sie\'c o tej architekturze jest w stanie nauczy\'c si\k e logicznej funkcji XOR.}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Przyk\IeC {\l }ad architektury rekurencyjnej. Zwr\'o\'cmy uwag\k e na skierowany cykl w grafie po\IeC {\l }\k acze\'n.}}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Wielowarstwowa sie\'c neuronowa jako uniwersalny aproksymator}{23}}
\citation{hornik}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Rysunek pomocniczy dla procesu wstecznej propagacji.}}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Algorytm wstecznej propagacji}{24}}
\citation{choromanska}
\@writefile{toc}{\contentsline {subsubsection}{Zjawisko znikaj\k acego gradientu (ang. \textit  {vanishing gradient problem})}{25}}
\newlabel{vanishing-gradien}{{2.3.3}{25}}
\citation{batch-normalization}
\@writefile{toc}{\contentsline {subsubsection}{Zjawisko przesuni\k ecia warto\'sci (ang. \textit  {covariate shift}) i technika normalizacji porcjowej (ang. \textit  {Batch Normalization})}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Interpretacja probabilistyczna sieci neuronowych}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Regularyzacja sieci neuronowych}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{2}$}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{1}$}{28}}
\citation{lasso}
\citation{curse-of-dimensionality}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Czym r\'o\.zni\k a si\k e powy\.zsze kary?}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Intuicje dotycz\k ace warstw sieci neuronowych}{29}}
\citation{zeiler-fergus}
\citation{unfolding-rnn}
\citation{zeiler-fergus}
\citation{bengio}
\citation{hierarchical}
\newlabel{cnn-feat}{{2.3.8}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Wizualizacja cech wyuczonych przez tzw. konwolucyjn\k a sie\'c neuronow\k a z \cite  {zeiler-fergus}.}}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}Reprezentacja rozproszona}{30}}
\@writefile{toc}{\contentsline {subsubsection}{G\IeC {\l }\k eboka hierarchiczna struktura przyczyn\k a sukcesu algorytm\'ow z rodziny Deep Learning}{30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Sieci konwolucyjne}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Inwariancja}{33}}
\citation{conv}
\newlabel{2d-conv}{{3.2}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Przyk\IeC {\l }ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z dwuwymiarowej macierzy oraz funkcji aktywacji.}}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Architektura sieci konwolucyjnych}{34}}
\newlabel{1d-conv}{{3.2}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Przyk\IeC {\l }ad jednowymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z wektora oraz funkcji aktywacji.}}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Najpopularniejsze rzutowania}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Przyk\IeC {\l }ad obrazuj\k acy dzia\IeC {\l }anie filtr\'ow konwolucyjnych.}}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Pooling}{36}}
\citation{receptive-field}
\citation{backprop}
\citation{lenet}
\citation{alex-net}
\citation{imagenet}
\newlabel{pool}{{3.2.2}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Przyk\IeC {\l }ad obrazuj\k acy dzia\IeC {\l }anie \textit  {poolingu}.}}{37}}
\@writefile{toc}{\contentsline {subsubsection}{Pooling globalny (ang. \textit  {global pooling})}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Przegl\k ad najpopularniejszych architektur konwolucyjnych}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Pierwsze sieci konwolucyjne}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}AlexNet}{37}}
\newlabel{lenet}{{3.3.1}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Schemat architektury sieci \textit  {LeNet-5}.}}{38}}
\newlabel{alex-net}{{3.3.2}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Schemat architektury sieci \textit  {AlexNet}.}}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Recitified Linear unit - ReLU}{38}}
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU}{38}}
\newlabel{relu}{{3.3.2}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Wykres funkcji i s\IeC {\l }abej pochodnej \textit  {ReLU}.}}{39}}
\newlabel{leaky-relu}{{3.3.2}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Wykres funkcji \textit  {Leaky ReLU}.}}{39}}
\@writefile{toc}{\contentsline {subsubsection}{Obliczenia na kartach graficznych}{39}}
\newlabel{gpu-speed-up}{{3.3.2}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Przyspieszanie kart graficznych oraz zwi\k ekszanie r\'o\.znicy pomi\k edzy obliczeniami wykonywanymi na GPU / CPU.}}{40}}
\@writefile{toc}{\contentsline {subsubsection}{Augmentacja obraz\'ow}{40}}
\citation{vgg}
\citation{imagenet}
\citation{inception}
\citation{inception-movie}
\newlabel{aug}{{3.3.2}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Przyk\IeC {\l }ady obrazk\'ow uzyskanych z pojedynczego obrazka ze zbioru treningowego przy pomocy losowych augmentacji.}}{41}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}VGG}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}GoogLeNet}{41}}
\newlabel{dropout}{{3.3.2}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Przyk\IeC {\l }ady architektury sieci w kt\'orej zastosowane technik\k e \textit  {dropout}.}}{42}}
\newlabel{vgg}{{3.3.3}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Schemat architektury sieci \textit  {VGG-19}.}}{42}}
\newlabel{inc-v1}{{3.3.4}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Schemat architektury sieci \textit  {Google Inception v1}}}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\IeC {\L }odyga (ang. \textit  {stem}) sieci}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Konwolucja $(f_w, f_h) = (1, 1)$ - redukcja wymiarowo\'sci}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Klasyfikatory pomocnicze}{43}}
\newlabel{auxiliary-classifiers}{{3.3.4}{43}}
\citation{batch-normalization}
\citation{inception-v3}
\citation{inception-v4}
\citation{resnet}
\newlabel{inception-block}{{3.3.4}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Przyk\IeC {\l }ad bloku incepcyjnego - \textit  {sieci w sieci}.}}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Blok incepcyjny - \textit  {sie\'c w sieci}}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Kolejne wersji Incepcji}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Sieci residualne ResNet}{44}}
\newlabel{residual-connections}{{3.3.5}{44}}
\@writefile{toc}{\contentsline {subsubsection}{Po\IeC {\l }\k aczenia residualne}{44}}
\newlabel{residual}{{3.3.5}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Przyk\IeC {\l }ad po\IeC {\l }\k aczenia residualnego.}}{45}}
\citation{nature-cancer}
\citation{nature-cancer}
\citation{wiki-cancer}
\citation{nature-cancer}
\citation{nature-cancer}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementacja - zastosowanie sieci konwolucyjnych do rozpoznawania nowotworu jelita grubego}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Opis problemu}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Opis zbioru danych i dotychczasowych wynik\'ow}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Zbi\'or danych}{47}}
\newlabel{dataset}{{4.2.1}{47}}
\citation{resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Przyk\IeC {\l }ady obrazk\'ow z ka\.zdej z klas.}}{48}}
\@writefile{toc}{\contentsline {paragraph}{Spos\'ob zebrania}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Opis sieci i eksperymentu}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Opis architektury sieci}{48}}
\citation{bilinear-interpolation}
\citation{yolo}
\@writefile{toc}{\contentsline {paragraph}{Wej\'scie}{49}}
\@writefile{toc}{\contentsline {paragraph}{\IeC {\L }odyga}{49}}
\@writefile{toc}{\contentsline {paragraph}{Blok 1}{49}}
\@writefile{toc}{\contentsline {paragraph}{Blok 2}{49}}
\@writefile{toc}{\contentsline {paragraph}{Interpretacja wyj\'s\'c}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Schemat bloku 1 i 2.}}{50}}
\@writefile{toc}{\contentsline {paragraph}{Normalizacja porcjowa oraz funkcja aktywacji}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Szczeg\'o\IeC {\l }y funkcji kosztu}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Szczeg\'o\IeC {\l }y algorytmu optymalizacyjnego}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Schemat bloku 1 i 2.}}{51}}
\citation{keras}
\citation{tensorflow}
\citation{azure-nc-6}
\citation{nature-cancer}
\citation{nature-cancer}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Szczeg\'o\IeC {\l }y procesu uczenia}{52}}
\newlabel{learning}{{4.3.4}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Szczeg\'o\IeC {\l }y implementacji}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Por\'ownanie wynik\'ow}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Wyniki uzyskane przez autor\'ow zbioru danych}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Wyniki uzyskane przez model opisany w tej pracy}{52}}
\@writefile{toc}{\contentsline {paragraph}{Szczeg\'o\IeC {\l }y uzyskiwania ostatecznych rezultat\'ow}{52}}
\citation{nature-cancer}
\citation{nature-cancer}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Macierz b\IeC {\l }\k ed\'ow uzyskana w kategorii klasyfikacji 8-klasowej przez \cite  {nature-cancer}.}}{53}}
\@writefile{toc}{\contentsline {subsubsection}{Por\'ownanie}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Macierz b\IeC {\l }\k ed\'ow uzyskana przez model opisany w tej pracy.}}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Krzywe ROC uzyskane przez autor\'ow \cite  {nature-cancer}.}}{54}}
\newlabel{roc-marcin}{{4.4.2}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Krzywe ROC uzyskane przez model opisany w tej pracy.}}{55}}
\citation{nature-cancer}
\citation{resnet}
\citation{inception}
\citation{wavenet}
\citation{deep-learning-translation}
\citation{variational-dropout}
\citation{active-learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Dyskusja}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{twarze}{1}
\bibcite{znaki}{2}
\bibcite{mnist}{3}
\bibcite{perceptron}{4}
\bibcite{mccul-pitts}{5}
\bibcite{minsky-papert}{6}
\bibcite{backprop}{7}
\bibcite{conv}{8}
\bibcite{popper}{9}
\bibcite{recurrent}{10}
\bibcite{cybenko}{11}
\bibcite{hornik}{12}
\bibcite{adam}{13}
\bibcite{minibatch}{14}
\bibcite{randomsearch}{15}
\bibcite{lasso}{16}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{59}}
\bibcite{bengio}{17}
\bibcite{sentiment}{18}
\bibcite{lem}{19}
\bibcite{vapnik-bet}{20}
\bibcite{manifold-learning}{21}
\bibcite{gradient-descents}{22}
\bibcite{nadam}{23}
\bibcite{loss-landscapes}{24}
\bibcite{logistic-regression}{25}
\bibcite{skip-connections}{26}
\bibcite{curse-of-dimensionality}{27}
\bibcite{unfolding-rnn}{28}
\bibcite{zeiler-fergus}{29}
\bibcite{hierarchical}{30}
\bibcite{res-net}{31}
\bibcite{batch-normalization}{32}
\bibcite{kullback-leibler}{33}
\bibcite{lenet}{34}
\bibcite{receptive-field}{35}
\bibcite{alex-net}{36}
\bibcite{imagenet}{37}
\bibcite{vgg}{38}
\bibcite{inception}{39}
\bibcite{inception-movie}{40}
\bibcite{inception-v3}{41}
\bibcite{inception-v4}{42}
\bibcite{resnet}{43}
\bibcite{dropout}{44}
\bibcite{nature-cancer}{45}
\bibcite{wiki-cancer}{46}
\bibcite{keras}{47}
\bibcite{tensorflow}{48}
\bibcite{azure-nc-6}{49}
\bibcite{choromanska}{50}
\bibcite{bilinear-interpolation}{51}
\bibcite{yolo}{52}
\bibcite{wavenet}{53}
\bibcite{deep-learning-translation}{54}
\bibcite{active-learning}{55}
\bibcite{variational-dropout}{56}
