\relax 
\catcode `"\active 
\select@language{polish}
\@writefile{toc}{\select@language{polish}}
\@writefile{lof}{\select@language{polish}}
\@writefile{lot}{\select@language{polish}}
\citation{twarze}
\citation{znaki}
\citation{sentiment}
\citation{mnist}
\citation{perceptron}
\citation{mccul-pitts}
\citation{minsky-papert}
\citation{minsky-papert}
\citation{lem}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Historia}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Pocz\k atki i perceptron}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Zima sztucznej inteligencji}{7}}
\citation{backprop}
\citation{backprop}
\citation{recurrent}
\citation{vapnik-bet}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Dlaczego perceptron zawi\'od\IeC {\l }?}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Algorytm wstecznej propagacji - odrodzenie i ponowna zima}{8}}
\citation{cybenko}
\citation{hornik}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Dlaczego sieci ponownie zawiod\IeC {\l }y?}{9}}
\citation{popper}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Matematyczne podstawy algorytm\'ow}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{matpod}{{2}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Uczenie}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formalna definicja uczenia}{11}}
\newlabel{defucz}{{2.1.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Uczenie z nadzorem}{12}}
\citation{manifold-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Uczenie bez nadzoru}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Generalizacja oraz przeuczenie}{14}}
\citation{gradient-descents}
\@writefile{toc}{\contentsline {subsubsection}{Zbiory walidacyjne i testowe}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Walidacja krzy\.zowa (ang. \textit  {cross-validation}}{15}}
\citation{adam}
\citation{nadam}
\citation{minibatch}
\citation{loss-landscapes}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Optymalizacja gradientowa}{16}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Uproszczony schemat optymalizacji gradientowej}}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Redukcja sta\IeC {\l }ej uczenia w wypadku wysokiej i sta\IeC {\l }ej warto\'sci funkcji kosztu (ang. \textit  {learning rate reduction on plateau})}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Stochastyczna optymalizacja gradientowa}{16}}
\citation{randomsearch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Metaoptymalizacja}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Sztuczne Sieci neuronowe}{17}}
\citation{logistic-regression}
\citation{TODO}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Biologiczna motywacja}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Regresja logistyczna}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoidalna funkcja aktywacji}{18}}
\newlabel{sigmoid}{{2.2.2}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Wykres funkcji sigmoidalnej.}}{19}}
\@writefile{toc}{\contentsline {subsubsection}{Logistyczna funkcja kosztu - (ang. \textit  {log-loss})}{19}}
\@writefile{toc}{\contentsline {subsubsection}{Uog\'olnienie funkcji \textit  {log-loss} do zagadnienia multiklasyfikacji}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Klasyczne sieci neuronowe oraz algorytm wstecznej propagacji}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Topologie sieci}{19}}
\citation{skip-connections}
\citation{cybenko}
\newlabel{siec}{{2.3.1}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Przyk\IeC {\l }ad architektury wartstwowej sieci typu \textit  {feed-forward}. Sie\'c o tej architekturze jest w stanie nauczy\'c si\k e logicznej funkcji XOR.}}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Wielowarstwowa sie\'c neuronowa jako uniwersalny aproksymator}{20}}
\citation{hornik}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Przyk\IeC {\l }ad architektury rekurencyjnej. Zwr\'o\'cmy uwag\k e na skierowany cykl w grafie po\IeC {\l }\k acze\'n.}}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Algorytm wstecznej propagacji}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Rysunek pomocniczy dla procesu wstecznej propagacji.}}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Zjawisko Covariance shift i technika Batch Normalization}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Interpretacja probabilistyczna sieci neuronowych}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Regularyzacja sieci neuronowych}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{2}$}{24}}
\citation{lasso}
\@writefile{toc}{\contentsline {subsubsection}{Kara zwi\k azana z $\delimiter 69645069 \Theta \delimiter 86422285 _{1}$}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Czym r\'o\.zni\k a si\k e powy\.zsze kary?}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Intuicje dotycz\k ace warstw sieci neuronowych}{25}}
\citation{curse-of-dimensionality}
\citation{zeiler-fergus}
\citation{unfolding-rnn}
\citation{zeiler-fergus}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}Reprezentacja rozproszona}{26}}
\citation{bengio}
\citation{hierarchical}
\newlabel{cnn-feat}{{2.3.8}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Wizualizacja cech wyuczonych przez tzw. konwolucyjn\k a sie\'c neuronow\k a z \cite  {zeiler-fergus}.}}{27}}
\@writefile{toc}{\contentsline {subsubsection}{G\IeC {\l }\k eboka hierarchiczna struktura przyczyn\k a sukcesu algorytm\'ow z rodziny Deep Learning}{27}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Rozw\'oj metod uczenia}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Sieci konwolucyjne oraz LeNet}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Inwariancja}{29}}
\citation{conv}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Sieci konwolucyjne}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Przyk\IeC {\l }ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z dwuwymiarowej macierzy oraz funkcji aktywacji.}}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Przyk\IeC {\l }ad dwuwymiarowej sieci konwolucyjnej. Obliczenia mo\.zna interpretowa\'c jako z\IeC {\l }o\.zenie splotu wag $w_1,\dots  ,w_n$ z warto\'sciami z wektora oraz funkcji aktywacji.}}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Pooling}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}LeNet}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Autoenkodery, sie\'c Hintona i Salakhudinowa}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Sie\'c Kryzhevskiego}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}GoogLeNet}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Inne sieci}{32}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Moja implementacja}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{twarze}{1}
\bibcite{znaki}{2}
\bibcite{mnist}{3}
\bibcite{perceptron}{4}
\bibcite{mccul-pitts}{5}
\bibcite{minsky-papert}{6}
\bibcite{backprop}{7}
\bibcite{conv}{8}
\bibcite{popper}{9}
\bibcite{recurrent}{10}
\bibcite{cybenko}{11}
\bibcite{hornik}{12}
\bibcite{adam}{13}
\bibcite{minibatch}{14}
\bibcite{randomsearch}{15}
\bibcite{lasso}{16}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{35}}
\bibcite{bengio}{17}
\bibcite{sentiment}{18}
\bibcite{lem}{19}
\bibcite{vapnik-bet}{20}
\bibcite{manifold-learning}{21}
\bibcite{gradient-descents}{22}
\bibcite{nadam}{23}
\bibcite{loss-landscapes}{24}
\bibcite{logistic-regression}{25}
\bibcite{skip-connections}{26}
\bibcite{curse-of-dimensionality}{27}
\bibcite{unfolding-rnn}{28}
\bibcite{zeiler-fergus}{29}
\bibcite{hierarchical}{30}
